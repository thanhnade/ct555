// Playbook Manager - Qu·∫£n l√Ω playbook v√† template K8s

function getClusterId() {
  let cid = window.currentClusterId;
  if (!cid) {
    try {
      const params = new URLSearchParams(window.location.search || '');
      const fromUrl = params.get('clusterId');
      if (fromUrl) {
        window.currentClusterId = fromUrl;
        cid = fromUrl;
      }
    } catch (_) { }
  }
  return cid;
}
// Load playbook cho cluster hi·ªán t·∫°i (optional override)
async function loadPlaybooks(clusterIdOverride) {
  const cid = clusterIdOverride || getClusterId();
  if (!cid) {
    console.error('No cluster selected');
    return;
  }

  try {
    // L∆∞u l·∫°i override n·∫øu c√≥
    if (clusterIdOverride) {
      window.currentClusterId = clusterIdOverride;
    }
    const response = await fetch(`/api/ansible-playbook/list/${cid}`);
    if (!response.ok) {
      throw new Error('Failed to load playbooks');
    }

    const playbooks = await response.json();
    const playbookList = document.getElementById('playbook-list');

    if (playbooks.length === 0) {
      playbookList.innerHTML = '<div class="list-group-item text-center text-muted">Ch∆∞a c√≥ playbook n√†o</div>';
    } else {
      playbookList.innerHTML = '';
      playbooks.forEach(pb => {
        const item = document.createElement('div');
        item.className = 'list-group-item d-flex justify-content-between align-items-center';
        item.innerHTML = `
          <div class="playbook-item" data-name="${pb}">
            <div class="fw-bold">${pb}</div>
            <div class="small text-muted">Playbook file</div>
          </div>
          <div class="btn-group btn-group-sm">
            <button class="btn btn-outline-primary btn-sm" onclick="loadPlaybook('${pb}')" title="Xem">
              üëÅÔ∏è
            </button>
            <button class="btn btn-outline-success btn-sm" onclick="executePlaybook('${pb}')" title="Th·ª±c thi">
              ‚ñ∂Ô∏è
            </button>
            <button class="btn btn-outline-danger btn-sm" onclick="deletePlaybook('${pb}')" title="X√≥a">
              üóëÔ∏è
            </button>
          </div>
        `;
        playbookList.appendChild(item);
      });
    }
  } catch (error) {
    console.error('Error loading playbooks:', error);
    // Hi·ªÉn th·ªã l·ªói trong playbook list thay v√¨ d√πng showAlert
    const playbookList = document.getElementById('playbook-list');
    if (playbookList) {
      playbookList.innerHTML = `
        <div class="list-group-item text-center text-danger">
          <i class="bi bi-exclamation-triangle me-2"></i>
          L·ªói t·∫£i danh s√°ch playbook: ${error.message}
        </div>
      `;
    }
  }
}

// T·∫£i n·ªôi dung playbook
window.loadPlaybook = async function (filename) {
  const cid = getClusterId();
  if (!cid || !filename) return;

  try {
    const response = await fetch(`/api/ansible-playbook/read/${cid}/${filename}`);
    if (!response.ok) {
      throw new Error('Failed to load playbook');
    }

    const data = await response.json();
    document.getElementById('playbook-filename').value = filename.replace('.yml', '');
    document.getElementById('playbook-editor').value = data.content;

    // Hi·ªÉn th·ªã view n·ªôi dung v√† ·∫©n view th·ª±c thi
    showPlaybookContentView();

  } catch (error) {
    console.error('Error loading playbook:', error);
    showAlert('error', 'L·ªói t·∫£i playbook: ' + error.message);
  }
};

// L∆∞u playbook
window.savePlaybook = async function () {
  const cid = getClusterId();
  if (!cid) return;

  const filename = document.getElementById('playbook-filename')?.value;
  const content = document.getElementById('playbook-editor')?.value;

  if (!filename || !content) {
    showAlert('error', 'Vui l√≤ng nh·∫≠p t√™n file v√† n·ªôi dung');
    return;
  }

  try {
    const response = await fetch(`/api/ansible-playbook/save/${cid}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
      },
      body: `filename=${encodeURIComponent(filename)}&content=${encodeURIComponent(content)}`
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.error || 'L·ªói l∆∞u playbook');
    }

    showAlert('success', 'ƒê√£ l∆∞u playbook th√†nh c√¥ng');
    await loadPlaybooks(); // C·∫≠p nh·∫≠t danh s√°ch

  } catch (error) {
    console.error('Error saving playbook:', error);
    showAlert('error', 'L·ªói l∆∞u playbook: ' + error.message);
  }
};

// X√≥a playbook
window.deletePlaybook = async function (filename) {
  const cid = getClusterId();
  if (!cid || !filename) return;

  if (!confirm(`X√≥a playbook "${filename}"?`)) return;

  try {
    const response = await fetch(`/api/ansible-playbook/delete/${cid}/${filename}`, {
      method: 'DELETE'
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.error || 'L·ªói x√≥a playbook');
    }

    showAlert('success', `ƒê√£ x√≥a playbook "${filename}" th√†nh c√¥ng `);
    await loadPlaybooks(); // C·∫≠p nh·∫≠t danh s√°ch

  } catch (error) {
    console.error('Error deleting playbook:', error);
    showAlert('error', 'L·ªói x√≥a playbook: ' + error.message);
  }
};

// Th·ª±c thi playbook
window.executePlaybook = async function (filename, extraVars = '') {
  const cid = getClusterId();
  if (!cid || !filename) return;

  try {
    // Hi·ªÉn th·ªã th·ª±c thi v√† ·∫©n n·ªôi dung
    showPlaybookExecutionView();

    const response = await fetch(`/api/ansible-playbook/execute/${cid}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
      },
      body: `filename=${encodeURIComponent(filename)}&extraVars=${encodeURIComponent(extraVars)}`
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.error || 'L·ªói th·ª±c thi playbook');
    }

    const result = await response.json();
    showAlert('success', `ƒê√£ b·∫Øt ƒë·∫ßu th·ª±c thi playbook: ${filename}`);

    // B·∫Øt ƒë·∫ßu theo d√µi tr·∫°ng th√°i th·ª±c thi
    if (result.taskId) {
      monitorPlaybookExecution(result.taskId);
    }

    return result;
  } catch (error) {
    console.error('Error executing playbook:', error);
    showAlert('error', 'L·ªói th·ª±c thi playbook: ' + error.message);
    showPlaybookContentView(); // Hi·ªÉn th·ªã n·ªôi dung khi c√≥ l·ªói
    throw error;
  }
};

// Theo d√µi th·ª±c thi playbook
async function monitorPlaybookExecution(taskId) {
  const outputElement = document.getElementById('ansible-output');
  const progressElement = document.getElementById('execution-progress');
  const spinnerElement = document.getElementById('execution-spinner');

  if (!outputElement || !progressElement || !spinnerElement) {
    console.error('Execution elements not found');
    return;
  }

  // X√≥a output tr∆∞·ªõc
  outputElement.innerHTML = '';

  // Hi·ªÉn th·ªã progress v√† spinner
  progressElement.style.display = 'block';
  spinnerElement.style.display = 'inline-block';

  const checkStatus = async () => {
    try {
      const response = await fetch(`/api/ansible-playbook/status/${getClusterId()}/${taskId}`);
      if (!response.ok) {
        throw new Error('Failed to check status');
      }

      const status = await response.json();

      // C·∫≠p nh·∫≠t progress bar
      const progressBar = progressElement.querySelector('.progress-bar');
      if (progressBar) {
        progressBar.style.width = `${status.progress || 0}%`;
        progressBar.setAttribute('aria-valuenow', status.progress || 0);
      }

      // C·∫≠p nh·∫≠t output
      if (status.output && status.output.length > 0) {
        const newOutput = status.output.slice(outputElement.children.length);
        newOutput.forEach(line => {
          const lineElement = document.createElement('div');
          lineElement.className = 'output-line';

          // M√£ h√≥a m√†u cho c√°c lo·∫°i output kh√°c nhau
          if (line.includes('TASK') || line.includes('PLAY')) {
            lineElement.className += ' task-header';
          } else if (line.includes('ok:') || line.includes('changed:')) {
            lineElement.className += ' success';
          } else if (line.includes('fatal:') || line.includes('failed:')) {
            lineElement.className += ' error';
          } else if (line.includes('skipping:')) {
            lineElement.className += ' warning';
          }

          lineElement.textContent = line;
          outputElement.appendChild(lineElement);
        });

        // Cu·ªôn xu·ªëng cu·ªëi
        outputElement.scrollTop = outputElement.scrollHeight;
      }

      if (status.status === 'running') {
        setTimeout(checkStatus, 1000);
      } else {
        // ·∫®n spinner
        spinnerElement.style.display = 'none';

        // Hi·ªÉn th·ªã th√¥ng b√°o ho√†n th√†nh
        const summaryElement = document.createElement('div');
        summaryElement.className = 'text-success mt-3 border-top pt-2';

        const titleElement = document.createElement('div');
        titleElement.className = 'fw-bold';
        titleElement.textContent = 'Ho√†n th√†nh th·ª±c thi playbook!';

        const timeElement = document.createElement('div');
        timeElement.className = 'small text-white';
        timeElement.textContent = `Th·ªùi gian th·ª±c thi: ${Math.round((status.endTime - status.startTime) / 1000)}s`;

        summaryElement.appendChild(titleElement);
        summaryElement.appendChild(timeElement);
        outputElement.appendChild(summaryElement);
      }

    } catch (error) {
      console.error('L·ªói theo d√µi th·ª±c thi:', error);
      spinnerElement.style.display = 'none';

      const errorElement = document.createElement('div');
      errorElement.className = 'text-danger mt-3';

      const errorTitle = document.createElement('div');
      errorTitle.className = 'fw-bold';
      errorTitle.textContent = 'L·ªói ki·ªÉm tra tr·∫°ng th√°i th·ª±c thi';

      errorElement.appendChild(errorTitle);
      outputElement.appendChild(errorElement);
    }
  };

  checkStatus();
}

// Ki·ªÉm tra xem playbook c√≥ t·ªìn t·∫°i kh√¥ng
async function checkPlaybookExists(filename) {
  const cid = getClusterId();
  if (!cid) {
    console.log('Kh√¥ng t√¨m th·∫•y ID cluster');
    return false;
  }

  try {
    const response = await fetch(`/api/ansible-playbook/list/${cid}`);
    if (!response.ok) {
      console.log('L·ªói t·∫£i danh s√°ch playbook:', response.status);
      return false;
    }

    const playbooks = await response.json();
    console.log('Playbook hi·ªán t·∫°i:', playbooks);
    console.log('T√¨m ki·∫øm:', filename);

    const exists = playbooks.includes(filename);
    console.log('File t·ªìn t·∫°i:', exists);
    return exists;
  } catch (error) {
    console.error('L·ªói ki·ªÉm tra playbook:', error);
    return false;
  }
}

// T·∫£i l√™n playbook
window.uploadPlaybook = async function (file) {
  const cid = getClusterId();
  if (!cid || !file) return;

  try {
    // Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
    const originalFilename = file.name;
    const finalFilename = originalFilename.toLowerCase().endsWith('.yml') || originalFilename.toLowerCase().endsWith('.yaml')
      ? originalFilename
      : originalFilename + '.yml';

    const exists = await checkPlaybookExists(finalFilename);

    if (exists) {
      const confirmMessage = `Playbook "${finalFilename}" ƒë√£ t·ªìn t·∫°i. B·∫°n c√≥ mu·ªën ghi ƒë√® l√™n file c≈© kh√¥ng?`;
      if (!confirm(confirmMessage)) {
        showAlert('info', 'ƒê√£ h·ªßy t·∫£i l√™n playbook');
        return;
      }
    }

    const formData = new FormData();
    formData.append('file', file);

    const response = await fetch(`/api/ansible-playbook/upload/${cid}`, {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.error || 'L·ªói t·∫£i l√™n playbook');
    }

    const result = await response.json();
    showAlert('success', `ƒê√£ t·∫£i l√™n playbook: ${result.filename}`);
    await loadPlaybooks(); // C·∫≠p nh·∫≠t danh s√°ch

    // T·∫£i n·ªôi dung playbook ƒë√£ t·∫£i l√™n
    await loadPlaybook(result.filename);

  } catch (error) {
    console.error('L·ªói t·∫£i l√™n playbook:', error);
    showAlert('error', 'L·ªói t·∫£i l√™n playbook: ' + error.message);
    throw error;
  }
};

// T·∫°o playbook K8s t·ª´ template
async function generateK8sPlaybookFromTemplate(template) {
  if (!getClusterId()) {
    throw new Error('Vui l√≤ng ch·ªçn cluster tr∆∞·ªõc');
  }

  const templates = {
    '01-update-hosts-hostname': `---
- name: Update /etc/hosts and hostname for entire cluster
  hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Add all inventory nodes to /etc/hosts
      lineinfile:
        path: /etc/hosts
        line: "{{ hostvars[item].ansible_host | default(item) }} {{ hostvars[item].ansible_user }}"
        state: present
        create: yes
        insertafter: EOF
      loop: "{{ groups['all'] }}"
      when: hostvars[item].ansible_user is defined
      tags: addhosts
      # Th√™m t·∫•t c·∫£ node trong inventory v√†o /etc/hosts
      
    - name: Set hostname according to inventory
      hostname:
        name: "{{ hostvars[inventory_hostname].ansible_user }}"
      when: ansible_hostname != hostvars[inventory_hostname].ansible_user
      tags: sethostname
      # ƒê·∫∑t hostname theo inventory
      
    - name: Verify hostname after update
      command: hostnamectl
      register: host_info
      changed_when: false
      tags: verify
      # Ki·ªÉm tra hostname sau khi c·∫≠p nh·∫≠t
      
    - name: Display information after update
      debug:
        msg:
          - "Hostname hi·ªán t·∫°i: {{ ansible_hostname }}"
          - "K·∫øt qu·∫£ l·ªánh hostnamectl:"
          - "{{ host_info.stdout_lines }}"
      tags: verify
      # Hi·ªÉn th·ªã th√¥ng tin sau khi c·∫≠p nh·∫≠t`,

    '02-kernel-sysctl': `---
- hosts: all
  become: yes
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Disable swap
      shell: swapoff -a || true
      ignore_errors: true
      # T·∫Øt swap v√¨ Kubernetes kh√¥ng h·ªó tr·ª£

    - name: Comment swap lines in /etc/fstab
      replace:
        path: /etc/fstab
        regexp: '(^.*swap.*$)'
        replace: '# \\1'
      # Comment d√≤ng swap trong /etc/fstab

    - name: Load kernel modules
      copy:
        dest: /etc/modules-load.d/containerd.conf
        content: |
          overlay
          br_netfilter
      # T·∫£i module kernel cho containerd

    - name: Load overlay and br_netfilter modules
      shell: |
        modprobe overlay
        modprobe br_netfilter
      # K√≠ch ho·∫°t module overlay v√† br_netfilter

    - name: Configure sysctl for Kubernetes
      copy:
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
      # C·∫•u h√¨nh sysctl cho Kubernetes

    - name: Apply sysctl configuration
      command: sysctl --system
      # √Åp d·ª•ng c·∫•u h√¨nh sysctl`,

    '03-install-containerd': `---
- hosts: all
  become: yes
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
      # C·∫≠p nh·∫≠t cache APT

    - name: Install containerd
      apt:
        name: containerd
        state: present
        force_apt_get: yes
      # C√†i ƒë·∫∑t containerd container runtime

    - name: Create containerd configuration directory
      file:
        path: /etc/containerd
        state: directory
      # T·∫°o th∆∞ m·ª•c c·∫•u h√¨nh containerd

    - name: Generate default containerd configuration
      shell: "containerd config default > /etc/containerd/config.toml"
      # T·∫°o file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd

    - name: Enable SystemdCgroup
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'
      # B·∫≠t SystemdCgroup trong containerd

    - name: Restart containerd service
      systemd:
        name: containerd
        enabled: yes
        state: restarted
      # Kh·ªüi ƒë·ªông l·∫°i containerd`,

    '04-install-kubernetes': `---
- hosts: all
  become: yes
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Install required packages
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
        state: present
        update_cache: yes
      # C√†i c√°c g√≥i ph·ª• thu·ªôc c·∫ßn thi·∫øt

    - name: Add Kubernetes GPG key
      shell: |
        if [ ! -f /usr/share/keyrings/kubernetes-archive-keyring.gpg ]; then
          curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \\
          gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
        else
          echo "GPG key ƒë√£ t·ªìn t·∫°i, b·ªè qua b∆∞·ªõc n√†y."
        fi
      changed_when: false
      register: gpg_status
      # Th√™m GPG key ch√≠nh th·ª©c c·ªßa Kubernetes

    - name: Add Kubernetes repository
      copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        content: |
          deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
      # Th√™m repository APT c·ªßa Kubernetes

    - name: Install kubelet, kubeadm, kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes
      # C√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn core c·ªßa Kubernetes

    - name: Hold package versions
      command: apt-mark hold kubelet kubeadm kubectl
      # Gi·ªØ phi√™n b·∫£n c√°c package ƒë·ªÉ tr√°nh c·∫≠p nh·∫≠t t·ª± ƒë·ªông`,

    '05-init-master': `---
- hosts: master
  become: yes
  gather_facts: yes
  environment:
    DEBIAN_FRONTEND: noninteractive

  vars:
    pod_network_cidr: "10.244.0.0/16"
    calico_manifest: "https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/calico.yaml"
    join_script: "/etc/kubernetes/join-command.sh"

  tasks:
    - name: Get dynamic master IP address
      set_fact:
        master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
      # L·∫•y ƒë·ªãa ch·ªâ IP ƒë·ªông c·ªßa master

    - name: Display master IP being used
      debug:
        msg: "S·ª≠ d·ª•ng ƒë·ªãa ch·ªâ master: {{ master_ip }}"
      # Hi·ªÉn th·ªã ƒë·ªãa ch·ªâ master ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng

    - name: Reset old cluster and clean up data
      shell: |
        kubeadm reset -f || true
        rm -rf /etc/kubernetes /var/lib/etcd /var/lib/kubelet /etc/cni/net.d
        systemctl restart containerd || true
      ignore_errors: yes
      # Reset c·ª•m c≈© v√† d·ªçn s·∫°ch d·ªØ li·ªáu

    - name: Initialize Kubernetes Control Plane
      command: >
        kubeadm init
        --control-plane-endpoint "{{ master_ip }}:6443"
        --apiserver-advertise-address {{ master_ip }}
        --pod-network-cidr {{ pod_network_cidr }}
        --upload-certs
      args:
        creates: /etc/kubernetes/admin.conf
      register: kubeadm_init
      failed_when: "'error' in kubeadm_init.stderr"
      changed_when: "'Your Kubernetes control-plane has initialized successfully' in kubeadm_init.stdout"
      # Kh·ªüi t·∫°o Control Plane v·ªõi Calico CNI

    - name: Configure kubeconfig for root user
      shell: |
        mkdir -p $HOME/.kube
        cp /etc/kubernetes/admin.conf $HOME/.kube/config
        chown $(id -u):$(id -g) $HOME/.kube/config
      args:
        executable: /bin/bash
      # C·∫•u h√¨nh kubeconfig cho root

    - name: Configure kubeconfig for normal user
      when: ansible_user != "root"
      block:
        - name: Create kubeconfig directory for user
          file:
            path: "/home/{{ ansible_user }}/.kube"
            state: directory
            mode: '0755'
          # T·∫°o th∆∞ m·ª•c kubeconfig cho user

        - name: Copy kubeconfig for user
          copy:
            src: /etc/kubernetes/admin.conf
            dest: "/home/{{ ansible_user }}/.kube/config"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0600'
            remote_src: yes
          # Sao ch√©p kubeconfig cho user
      # C·∫•u h√¨nh kubeconfig cho user th∆∞·ªùng (n·∫øu kh√¥ng ph·∫£i root)

    - name: Generate join command for workers
      shell: kubeadm token create --print-join-command
      register: join_cmd
      changed_when: false
      # Sinh l·ªánh join cho worker

    - name: Save join command to file
      copy:
        content: "{{ join_cmd.stdout }}"
        dest: "{{ join_script }}"
        mode: '0755'
      # L∆∞u l·ªánh join ra file

    - name: Display join command
      debug:
        msg:
          - "L·ªánh join worker:"
          - "{{ join_cmd.stdout }}"
          - "File l∆∞u t·∫°i: {{ join_script }}"
      # Hi·ªÉn th·ªã join command

    - name: Complete master initialization
      debug:
        msg: "Master {{ inventory_hostname }} ƒë√£ s·∫µn s√†ng cho worker join!"
      # Ho√†n t·∫•t kh·ªüi t·∫°o master`,

    '06-install-cni': `---
- name: Install or update Calico CNI (automatic)
  hosts: master
  become: yes
  gather_facts: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    DEBIAN_FRONTEND: noninteractive

  vars:
    calico_version: "v3.27.3"
    calico_url: "https://raw.githubusercontent.com/projectcalico/calico/{{ calico_version }}/manifests/calico.yaml"

  tasks:
    - name: Check if Calico CNI exists
      command: kubectl get daemonset calico-node -n kube-system
      register: calico_check
      ignore_errors: true

    - name: Display current status
      debug:
        msg: >
          {% if calico_check.rc == 0 %}
            Calico ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.
          {% else %}
            Ch∆∞a c√≥ Calico, s·∫Ω ti·∫øn h√†nh c√†i ƒë·∫∑t m·ªõi.
          {% endif %}

    - name: Verify kernel modules overlay and br_netfilter
      shell: |
        modprobe overlay || true
        modprobe br_netfilter || true
        lsmod | grep -E 'overlay|br_netfilter' || echo "Thi·∫øu module kernel"
      register: kernel_status
      ignore_errors: true

    - name: Display kernel module check result
      debug:
        var: kernel_status.stdout_lines

    - name: Verify sysctl configuration
      shell: |
        echo "net.bridge.bridge-nf-call-iptables = 1" | tee /etc/sysctl.d/k8s.conf >/dev/null
        echo "net.ipv4.ip_forward = 1" | tee -a /etc/sysctl.d/k8s.conf >/dev/null
        sysctl --system | grep -E "net.bridge.bridge-nf-call|net.ipv4.ip_forward"
      register: sysctl_status
      ignore_errors: true

    - name: Display sysctl result
      debug:
        var: sysctl_status.stdout_lines

    - name: Apply Calico manifest (install or update)
      shell: |
        kubectl apply -f {{ calico_url }}
      args:
        executable: /bin/bash
      register: calico_apply
      retries: 3
      delay: 10
      until: calico_apply.rc == 0

    - name: Display installation result
      debug:
        var: calico_apply.stdout_lines

    - name: Check Calico node pods running
      shell: |
        kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c 'Running' || true
      register: calico_running

    - name: Wait for pods to start (max 10 attempts)
      until: calico_running.stdout | int > 0
      retries: 10
      delay: 15
      shell: |
        kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c 'Running' || true
      register: calico_running
      ignore_errors: true

    - name: Confirm Calico pods are running
      when: calico_running.stdout | int > 0
      debug:
        msg: "Calico ƒëang ho·∫°t ƒë·ªông ({{ calico_running.stdout }} pods Running)."

    - name: Log Calico pods if failed
      when: calico_running.stdout | int == 0
      shell: kubectl logs -n kube-system -l k8s-app=calico-node --tail=50 || true
      register: calico_logs
      ignore_errors: true

    - name: Display Calico pod logs
      when: calico_running.stdout | int == 0
      debug:
        msg: "{{ calico_logs.stdout_lines | default(['Pod Calico ch∆∞a s·∫µn s√†ng ho·∫∑c kh√¥ng c√≥ log.']) }}"

    - name: Check node status
      command: kubectl get nodes -o wide
      register: nodes_status
      ignore_errors: true

    - name: Display cluster result
      debug:
        var: nodes_status.stdout_lines`,

    '06-install-flannel': `---
- name: Install or update Flannel CNI (WSL2 compatible)
  hosts: master
  become: yes
  gather_facts: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    DEBIAN_FRONTEND: noninteractive

  vars:
    flannel_manifest: "https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml"

  tasks:
    - name: Check if Flannel CNI exists
      command: kubectl get daemonset kube-flannel-ds -n kube-flannel
      register: flannel_check
      ignore_errors: true

    - name: Display current status
      debug:
        msg: >
          {% if flannel_check.rc == 0 %}
            Flannel ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t tr∆∞·ªõc ƒë√≥.
          {% else %}
            Ch∆∞a c√≥ Flannel, s·∫Ω ti·∫øn h√†nh c√†i ƒë·∫∑t m·ªõi.
          {% endif %}

    - name: Enable IP forwarding
      shell: |
        echo "net.ipv4.ip_forward = 1" | tee /etc/sysctl.d/k8s.conf >/dev/null
        sysctl --system | grep net.ipv4.ip_forward
      register: sysctl_status
      ignore_errors: true

    - name: Display sysctl result
      debug:
        var: sysctl_status.stdout_lines

    - name: Apply Flannel manifest (auto latest)
      command: kubectl apply -f {{ flannel_manifest }}
      register: flannel_apply
      changed_when: "'created' in flannel_apply.stdout or 'configured' in flannel_apply.stdout"
      failed_when: flannel_apply.rc != 0

    - name: Display apply result
      debug:
        var: flannel_apply.stdout_lines

    - name: Check number of running Flannel pods
      shell: |
        kubectl get pods -n kube-flannel --no-headers 2>/dev/null | grep -c 'Running' || true
      register: flannel_running

    - name: Wait for Flannel pods to be running (max 10 attempts)
      until: flannel_running.stdout | int > 0
      retries: 10
      delay: 15
      shell: |
        kubectl get pods -n kube-flannel --no-headers 2>/dev/null | grep -c 'Running' || true
      register: flannel_running
      ignore_errors: true

    - name: Confirm Flannel pods are running
      when: flannel_running.stdout | int > 0
      debug:
        msg: "Flannel ƒëang ho·∫°t ƒë·ªông ({{ flannel_running.stdout }} pods Running)."

    - name: Log Flannel if pods not running
      when: flannel_running.stdout | int == 0
      shell: kubectl logs -n kube-flannel -l app=flannel --tail=50 || true
      register: flannel_logs
      ignore_errors: true

    - name: Display Flannel logs
      when: flannel_running.stdout | int == 0
      debug:
        msg: "{{ flannel_logs.stdout_lines | default(['Pod Flannel ch∆∞a s·∫µn s√†ng ho·∫∑c kh√¥ng c√≥ log.']) }}"

    - name: Check node status
      command: kubectl get nodes -o wide
      register: nodes_status
      ignore_errors: true

    - name: Display cluster result
      debug:
        var: nodes_status.stdout_lines`,

    '07-join-workers': `---
- hosts: workers
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  vars:
    join_script: /tmp/kube_join.sh
  tasks:
    - name: Test SSH connectivity to worker node
      ping:
      register: ping_result
      ignore_errors: yes
      # Ki·ªÉm tra k·∫øt n·ªëi SSH ƒë·∫øn worker node

    - name: Skip offline workers
      set_fact:
        worker_online: "{{ ping_result is succeeded }}"
      # ƒê√°nh d·∫•u worker n√†o online/offline

    - name: Display worker online status
      debug:
        msg: "Worker {{ inventory_hostname }} is {{ 'ONLINE' if worker_online else 'OFFLINE' }}"
      # Hi·ªÉn th·ªã tr·∫°ng th√°i online/offline

    - name: Get join command from master
      delegate_to: "{{ groups['master'][0] }}"
      run_once: true
      shell: kubeadm token create --print-join-command
      register: join_cmd
      when: worker_online
      # L·∫•y l·ªánh join t·ª´ master node (ch·ªâ ch·∫°y 1 l·∫ßn)

    - name: Save join command to file
      copy:
        content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
        dest: "{{ join_script }}"
        mode: '0755'
      when: worker_online
      ignore_errors: yes
      # Ghi l·ªánh join ra file script

    - name: Reset node if old cluster exists
      shell: |
        kubeadm reset -f || true
        rm -rf /etc/kubernetes /var/lib/kubelet /etc/cni/net.d
        systemctl restart containerd || true
      ignore_errors: yes
      when: worker_online
      # Reset node c≈© (n·∫øu c√≥)

    - name: Join to Kubernetes cluster
      shell: "{{ join_script }}"
      register: join_output
      ignore_errors: yes
      when: worker_online
      # Th·ª±c thi l·ªánh join v√†o cluster

    - name: Display join result
      debug:
        msg: "{{ join_output.stdout_lines | default(['ƒê√£ join th√†nh c√¥ng!']) if worker_online else ['Worker offline, skip join'] }}"
      # Hi·ªÉn th·ªã k·∫øt qu·∫£ join

    - name: Restart kubelet service
      systemd:
        name: kubelet
        state: restarted
        enabled: yes
      ignore_errors: yes
      when: worker_online
      # Kh·ªüi ƒë·ªông l·∫°i kubelet

    - name: Complete join process
      debug:
        msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"
      # B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng`,

    '09-install-ingress': `---
- hosts: master
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive

  tasks:
    - name: Get dynamic master IP address
      set_fact:
        master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
    - debug:
        msg: "C√†i ƒë·∫∑t Ingress tr√™n master: {{ master_ip }}"

    - name: Install Ingress Controller (NGINX)
      shell: |
        KUBECONFIG=/etc/kubernetes/admin.conf \
        kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
      args:
        executable: /bin/bash
      register: ingress_install
      ignore_errors: yes

    - name: Display Ingress installation result
      debug:
        msg: "{{ ingress_install.stdout_lines | default(['Ingress Controller applied']) }}"

    - name: Check ingress-nginx pod status
      shell: |
        KUBECONFIG=/etc/kubernetes/admin.conf \
        kubectl get pods -n ingress-nginx -o wide
      register: ingress_pods

    - name: Display ingress-nginx pods
      debug:
        msg: "{{ ingress_pods.stdout_lines }}"

    - name: Complete
      debug:
        msg: "Ingress Controller (NGINX) ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t th√†nh c√¥ng!"`,

    '10-install-helm': `---
- hosts: master
  become: yes
  gather_facts: yes
  environment:
    DEBIAN_FRONTEND: noninteractive

  tasks:
    - name: Install Helm if missing
      shell: |
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      args:
        executable: /bin/bash
      register: helm_install
      ignore_errors: yes

    - name: Display Helm installation result
      debug:
        msg: "{{ helm_install.stdout_lines | default(['Helm installed']) }}"

    - name: Check Helm version
      shell: helm version --short
      register: helm_version

    - name: Display Helm info
      debug:
        msg: "Phi√™n b·∫£n Helm hi·ªán t·∫°i: {{ helm_version.stdout | default('Kh√¥ng x√°c ƒë·ªãnh') }}"

    - name: Complete
      debug:
        msg: "Helm ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t th√†nh c√¥ng tr√™n master!"`,

    '11-setup-storage': `---
- hosts: master
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive

  vars:
    nfs_manifest_dir: /etc/kubernetes/storage

  tasks:
    - name: Create NFS manifest directory
      file:
        path: "{{ nfs_manifest_dir }}"
        state: directory
        mode: '0755'

    - name: Download and apply NFS Provisioner (example)
      shell: |
        KUBECONFIG=/etc/kubernetes/admin.conf \
        kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/rbac.yaml
        KUBECONFIG=/etc/kubernetes/admin.conf \
        kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/deployment.yaml
      args:
        executable: /bin/bash
      register: nfs_apply
      ignore_errors: yes

    - name: Display NFS deployment result
      debug:
        msg: "{{ nfs_apply.stdout_lines | default(['NFS Provisioner applied']) }}"

    - name: Set default StorageClass
      shell: |
        KUBECONFIG=/etc/kubernetes/admin.conf \
        kubectl patch storageclass nfs-client -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' || true
      args:
        executable: /bin/bash

    - name: Complete
      debug:
        msg: "C·∫•u h√¨nh StorageClass (NFS) m·∫∑c ƒë·ªãnh ƒë√£ ho√†n t·∫•t!"`,

    '12-prepare-and-join-worker': `---
# All-in-one: Chu·∫©n b·ªã node v√† join v√†o c·ª•m (02 ‚Üí 03 ‚Üí 04 ‚Üí 07)

# Precheck: Ch·ªâ ƒë·ªãnh nh√≥m target_workers g·ªìm c√°c worker ch∆∞a Ready ho·∫∑c ch∆∞a c√≥ trong kubectl
- name: Precheck not-ready workers
  hosts: master
  gather_facts: no
  tasks:
    - name: Check if kubectl is available
      command: which kubectl
      register: kubectl_check
      failed_when: false
      changed_when: false

    - name: Get existing node list
      command: kubectl get nodes --no-headers
      register: nodes_tbl
      changed_when: false
      failed_when: false
      when: kubectl_check.rc == 0

    - name: Ensure node_lines fact always exists
      set_fact:
        node_lines: "{{ nodes_tbl.stdout_lines | default([]) if (nodes_tbl is defined and nodes_tbl.stdout_lines is defined) else [] }}"

    - name: Parse node information safely
      set_fact:
        node_names: "{{ node_lines | map('split') | map('first') | list | default([]) }}"
        not_ready_names: "{{ node_lines | map('split') | selectattr('1','ne','Ready') | map('first') | list | default([]) }}"

    - name: Add unregistered or not-ready workers to target group
      add_host:
        name: "{{ item }}"
        groups: target_workers
      loop: "{{ groups['workers'] | default([]) }}"
      when: kubectl_check.rc != 0 or
            nodes_tbl is not defined or
            item not in (node_names | default([])) or
            item in (not_ready_names | default([]))

- name: 02 - Configure kernel and sysctl
  hosts: target_workers
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Disable swap
      shell: swapoff -a || true
      ignore_errors: true

    - name: Comment swap entries in /etc/fstab
      replace:
        path: /etc/fstab
        regexp: '(^.*swap.*$)'
        replace: '# \\1'
      ignore_errors: yes

    - name: Create modules-load file for containerd
      copy:
        dest: /etc/modules-load.d/containerd.conf
        content: |
          overlay
          br_netfilter

    - name: Enable overlay and br_netfilter modules
      shell: |
        modprobe overlay || true
        modprobe br_netfilter || true

    - name: Configure sysctl for Kubernetes
      copy:
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1

    - name: Apply sysctl
      command: sysctl --system
      ignore_errors: yes

- name: 03 - Install and configure containerd
  hosts: target_workers
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: C·∫≠p nh·∫≠t cache APT
      apt:
        update_cache: yes

    - name: C√†i containerd
      apt:
        name: containerd
        state: present
        force_apt_get: yes

    - name: T·∫°o th∆∞ m·ª•c c·∫•u h√¨nh containerd
      file:
        path: /etc/containerd
        state: directory

    - name: Sinh file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd
      shell: "containerd config default > /etc/containerd/config.toml"

    - name: B·∫≠t SystemdCgroup
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'

    - name: Kh·ªüi ƒë·ªông l·∫°i containerd
      systemd:
        name: containerd
        enabled: yes
        state: restarted

- name: 04 - Install Kubernetes (kubelet, kubeadm, kubectl)
  hosts: target_workers
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Add Kubernetes GPG key (if missing)
      shell: |
        if [ ! -f /usr/share/keyrings/kubernetes-archive-keyring.gpg ]; then
          curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
          gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
        fi
      changed_when: false
      ignore_errors: true

    - name: Add Kubernetes repository
      copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        content: |
          deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /

    - name: Install kubelet, kubeadm, kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes

    - name: Hold kubelet/kubeadm/kubectl versions
      command: apt-mark hold kubelet kubeadm kubectl

- name: 07 - Join workers to cluster
  hosts: target_workers
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  vars:
    join_script: /tmp/kube_join.sh
  tasks:
    - name: Check SSH connectivity to worker
      ping:
      register: ping_result
      ignore_errors: yes

    - name: Mark online status
      set_fact:
        worker_online: "{{ ping_result is succeeded }}"

    - name: Get join command from master
      delegate_to: "{{ groups['master'][0] }}"
      run_once: true
      shell: kubeadm token create --print-join-command
      register: join_cmd
      when: worker_online

    - name: Write join command to file
      copy:
        content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
        dest: "{{ join_script }}"
        mode: '0755'
      when: worker_online
      ignore_errors: yes

    - name: Reset old node (if exists)
      shell: |
        kubeadm reset -f || true
        rm -rf /etc/kubernetes /var/lib/kubelet /etc/cni/net.d
        systemctl restart containerd || true
      ignore_errors: yes
      when: worker_online

    - name: Execute join command
      shell: "{{ join_script }}"
      register: join_output
      ignore_errors: yes
      when: worker_online

    - name: Restart kubelet
      systemd:
        name: kubelet
        state: restarted
        enabled: yes
      ignore_errors: yes
      when: worker_online

    - name: Summarize results
      debug:
        msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"`,



    '08-verify-cluster': `---
- name: Verify Kubernetes cluster status
  hosts: master
  become: yes
  gather_facts: no
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    DEBIAN_FRONTEND: noninteractive

  tasks:
    - name: Check kubectl availability
      command: which kubectl
      register: kubectl_check
      failed_when: kubectl_check.rc != 0
      changed_when: false

    - name: List nodes
      command: kubectl get nodes
      register: nodes_info
      changed_when: false

    - name: List system pods
      command: kubectl get pods -n kube-system
      register: pods_info
      changed_when: false

    - name: Display cluster information
      debug:
        msg:
          - "Danh s√°ch Node:"
          - "{{ nodes_info.stdout_lines }}"
          - "Pods trong namespace kube-system:"
          - "{{ pods_info.stdout_lines }}"

    - name: Check node status
      shell: kubectl get nodes --no-headers | awk '{print $2}' | sort | uniq -c
      register: node_status
      changed_when: false

    - name: Report node status
      debug:
        msg: |
          {% if 'NotReady' in node_status.stdout %}
          M·ªôt s·ªë node ch∆∞a s·∫µn s√†ng:
          {{ node_status.stdout }}
          {% else %}
          T·∫•t c·∫£ node ƒë√£ ·ªü tr·∫°ng th√°i Ready!
          {% endif %}

    - name: Check failing pods in kube-system
      shell: kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' || true
      register: bad_pods
      changed_when: false

    - name: Report problematic pods
      debug:
        msg: |
          {% if bad_pods.stdout %}
          M·ªôt s·ªë pod ch∆∞a ·ªïn ƒë·ªãnh ho·∫∑c ƒëang l·ªói:
          {{ bad_pods.stdout }}
          {% else %}
          T·∫•t c·∫£ pod trong kube-system ƒë·ªÅu ƒëang Running ho·∫∑c Completed!
          {% endif %}

    - name: Show logs of problematic pods (if any)
      when: bad_pods.stdout != ""
      shell: |
        for pod in $(kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' | awk '{print $1}'); do
          echo "Log c·ªßa $pod:"; kubectl logs -n kube-system $pod --tail=30 || true; echo "--------------------------------";
        done
      register: bad_pods_logs
      ignore_errors: yes

    - name: Detailed logs
      when: bad_pods.stdout != ""
      debug:
        msg: "{{ bad_pods_logs.stdout_lines | default(['Kh√¥ng c√≥ log l·ªói']) }}"`,

    '00-reset-cluster': `---
- name: Reset entire Kubernetes cluster
  hosts: all
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Reset Kubernetes cluster
      shell: kubeadm reset -f
      ignore_errors: true
      register: reset_output
      # G·ª° c·ª•m Kubernetes (kubeadm reset -f)

    - name: Display reset results
      debug:
        msg: "{{ reset_output.stdout_lines | default(['Kh√¥ng c√≥ cluster c≈© ƒë·ªÉ reset.']) }}"
      # Hi·ªÉn th·ªã k·∫øt qu·∫£ reset

    - name: Remove Kubernetes configuration directory
      file:
        path: /etc/kubernetes
        state: absent
      # X√≥a th∆∞ m·ª•c c·∫•u h√¨nh Kubernetes

    - name: Remove CNI network configuration
      file:
        path: /etc/cni/net.d
        state: absent
      # X√≥a c·∫•u h√¨nh m·∫°ng CNI

    - name: Remove root kubeconfig
      file:
        path: /root/.kube
        state: absent
      # X√≥a file kubeconfig c·ªßa root

    - name: Remove normal user kubeconfig
      file:
        path: "/home/{{ ansible_user }}/.kube"
        state: absent
      when: ansible_user != "root"
      # X√≥a file kubeconfig c·ªßa user th∆∞·ªùng

    - name: Clean up iptables rules
      shell: |
        iptables -F && iptables -X
        iptables -t nat -F && iptables -t nat -X
        iptables -t mangle -F && iptables -t mangle -X
        iptables -P FORWARD ACCEPT
      ignore_errors: true
      # D·ªçn iptables

    - name: Restart containerd service
      systemd:
        name: containerd
        state: restarted
        enabled: yes
      # Kh·ªüi ƒë·ªông l·∫°i containerd

    - name: Confirm reset completed
      debug:
        msg:
          - "Node {{ inventory_hostname }} ƒë√£ ƒë∆∞·ª£c reset s·∫°ch (ch·ªâ x√≥a d·ªØ li·ªáu)."
      # X√°c nh·∫≠n reset ho√†n t·∫•t`,

    'deploy-full-cluster': `---
- name: Step 0 - Reset Kubernetes cluster
  hosts: all
  become: yes
  gather_facts: yes
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Reset old cluster if exists
      shell: kubeadm reset -f || true
      ignore_errors: true
      # Reset c·ª•m Kubernetes c≈© n·∫øu t·ªìn t·∫°i

    - name: Remove old Kubernetes configuration
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes
        - /etc/cni/net.d
        - /root/.kube
        - "/home/{{ ansible_user }}/.kube"
      ignore_errors: true
      # X√≥a c√°c th∆∞ m·ª•c c·∫•u h√¨nh Kubernetes c≈©

    - name: Restart containerd service
      shell: systemctl restart containerd || true
      ignore_errors: true
      # Kh·ªüi ƒë·ªông l·∫°i containerd ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch s·∫Ω

- name: Step 1 - Update /etc/hosts and hostname
  hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Update /etc/hosts file for all nodes
      lineinfile:
        path: /etc/hosts
        line: "{{ hostvars[item].ansible_host }} {{ item }}"
        state: present
        create: yes
        insertafter: EOF
      loop: "{{ groups['all'] }}"
      when: hostvars[item].ansible_host is defined
      # Th√™m danh s√°ch c√°c node v√†o /etc/hosts ƒë·ªÉ cluster nh·∫≠n di·ªán nhau

    - name: Set hostname according to inventory
      hostname:
        name: "{{ inventory_hostname }}"
      when: ansible_hostname != inventory_hostname
      # ƒê·∫∑t hostname theo t√™n trong inventory

    - name: Verify hostname
      shell: hostnamectl
      register: host_info
      # Ki·ªÉm tra hostname ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t ƒë√∫ng

    - name: Display hostname info
      debug:
        msg: "{{ host_info.stdout_lines }}"
      # Hi·ªÉn th·ªã th√¥ng tin hostname

- name: Step 2 - Configure kernel and containerd
  hosts: all
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Disable swap
      shell: swapoff -a && sed -i '/swap/d' /etc/fstab || true
      # T·∫Øt swap v√¨ Kubernetes kh√¥ng h·ªó tr·ª£

    - name: Load kernel modules for containerd
      copy:
        dest: /etc/modules-load.d/containerd.conf
        content: |
          overlay
          br_netfilter
      # T·∫£i c√°c kernel module c·∫ßn thi·∫øt cho containerd

    - name: Activate kernel modules
      shell: |
        modprobe overlay || true
        modprobe br_netfilter || true
      # K√≠ch ho·∫°t module overlay v√† br_netfilter

    - name: Configure sysctl parameters for Kubernetes
      copy:
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
      # C·∫•u h√¨nh sysctl cho networking Kubernetes

    - name: Apply sysctl configuration
      command: sysctl --system
      # √Åp d·ª•ng c·∫•u h√¨nh sysctl

    - name: Install containerd runtime
      apt:
        name: containerd
        state: present
        update_cache: yes
      # C√†i ƒë·∫∑t containerd container runtime

    - name: Generate default containerd config
      shell: |
        mkdir -p /etc/containerd
        containerd config default > /etc/containerd/config.toml
      # T·∫°o file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd

    - name: Enable SystemdCgroup
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'
      # B·∫≠t SystemdCgroup trong containerd

    - name: Restart and enable containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes
      # Kh·ªüi ƒë·ªông l·∫°i v√† b·∫≠t containerd ƒë·ªÉ √°p d·ª•ng c·∫•u h√¨nh

- name: Step 3 - Install Kubernetes core components
  hosts: all
  become: yes
  gather_facts: no
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Add Kubernetes GPG key
      shell: |
        mkdir -p /usr/share/keyrings
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
        gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
      changed_when: false
      ignore_errors: true
      # Th√™m GPG key ch√≠nh th·ª©c c·ªßa Kubernetes

    - name: Add Kubernetes repository
      copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        content: |
          deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
      # Th√™m repository APT c·ªßa Kubernetes

    - name: Install kubelet, kubeadm, kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes
      # C√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn core c·ªßa Kubernetes

    - name: Hold package version
      command: apt-mark hold kubelet kubeadm kubectl
      # Gi·ªØ phi√™n b·∫£n c√°c package ƒë·ªÉ tr√°nh c·∫≠p nh·∫≠t t·ª± ƒë·ªông

- name: Step 4 - Initialize Master node
  hosts: master
  become: yes
  gather_facts: yes
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Get master IP address
      set_fact:
        master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
      # L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa master node

    - name: Reset old control plane (if any)
      shell: kubeadm reset -f || true
      # Reset control plane c≈© n·∫øu c√≥

    - name: Initialize Kubernetes control plane
      command: >
        kubeadm init
        --control-plane-endpoint "{{ master_ip }}:6443"
        --apiserver-advertise-address {{ master_ip }}
        --pod-network-cidr 10.244.0.0/16
        --upload-certs
      args:
        creates: /etc/kubernetes/admin.conf
      register: kubeadm_init
      failed_when: "'error' in kubeadm_init.stderr"
      changed_when: "'Your Kubernetes control-plane has initialized successfully' in kubeadm_init.stdout"
      # Kh·ªüi t·∫°o control plane Kubernetes v·ªõi Calico CNI

    - name: Copy kubeconfig for root
      shell: |
        mkdir -p /root/.kube
        cp -i /etc/kubernetes/admin.conf /root/.kube/config
        chown root:root /root/.kube/config
      args:
        executable: /bin/bash
      # Sao ch√©p kubeconfig cho user root

    - name: Copy kubeconfig for normal user
      when: ansible_user != "root"
      block:
        - name: Create ~/.kube directory
          file:
            path: "/home/{{ ansible_user }}/.kube"
            state: directory
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0755'
          # T·∫°o th∆∞ m·ª•c kubeconfig cho user th∆∞·ªùng

        - name: Copy kubeconfig file
          copy:
            src: /etc/kubernetes/admin.conf
            dest: "/home/{{ ansible_user }}/.kube/config"
            remote_src: yes
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0600'
          # Sao ch√©p kubeconfig cho user th∆∞·ªùng
      # Sao ch√©p kubeconfig cho user th∆∞·ªùng (n·∫øu kh√¥ng ph·∫£i root)

- name: Step 5 - Install Calico CNI
  hosts: master
  become: yes
  gather_facts: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    DEBIAN_FRONTEND: noninteractive
  vars:
    calico_version: "v3.27.3"
    calico_url: "https://raw.githubusercontent.com/projectcalico/calico/{{ calico_version }}/manifests/calico.yaml"
  tasks:
    - name: Check if Calico CNI exists
      command: kubectl get daemonset calico-node -n kube-system
      register: calico_check
      ignore_errors: true
      # Ki·ªÉm tra Calico ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t tr∆∞·ªõc ƒë√≥

    - name: Apply Calico manifest (install or update)
      command: kubectl apply -f {{ calico_url }}
      register: calico_apply
      retries: 3
      delay: 10
      until: calico_apply.rc == 0
      # √Åp d·ª•ng manifest Calico CNI

    - name: Wait for Calico node pods to be Running
      shell: |
        kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c 'Running' || true
      register: calico_running
      retries: 10
      delay: 15
      until: calico_running.stdout | int > 0
      # Ch·ªù c√°c pod Calico kh·ªüi ƒë·ªông v√† ·ªü tr·∫°ng th√°i Running

    - name: Confirm Calico pods are active
      debug:
        msg: "Calico is running ({{ calico_running.stdout }} pods Running)."
      # X√°c nh·∫≠n Calico ƒëang ho·∫°t ƒë·ªông

- name: Step 6 - Join worker nodes
  hosts: workers
  become: yes
  gather_facts: false
  environment:
    DEBIAN_FRONTEND: noninteractive
  vars:
    join_script: /tmp/kube_join.sh
  tasks:
    - name: Test SSH connectivity to worker node
      ping:
      register: ping_result
      ignore_errors: yes
      # Ki·ªÉm tra k·∫øt n·ªëi SSH ƒë·∫øn worker node

    - name: Mark worker online status
      set_fact:
        worker_online: "{{ ping_result is succeeded }}"
      # ƒê√°nh d·∫•u worker n√†o online/offline

    - name: Display worker online status
      debug:
        msg: "Worker {{ inventory_hostname }} is {{ 'ONLINE' if worker_online else 'OFFLINE' }}"
      # Hi·ªÉn th·ªã tr·∫°ng th√°i online/offline

    - name: Retrieve join command from master
      delegate_to: "{{ groups['master'][0] }}"
      run_once: true
      shell: kubeadm token create --print-join-command
      register: join_cmd
      when: worker_online
      # L·∫•y l·ªánh join t·ª´ master node (ch·ªâ ch·∫°y 1 l·∫ßn)

    - name: Save join command to script file
      copy:
        content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
        dest: "{{ join_script }}"
        mode: '0755'
      when: worker_online
      ignore_errors: yes
      # L∆∞u l·ªánh join v√†o file script

    - name: Reset old worker node
      shell: kubeadm reset -f || true
      ignore_errors: yes
      when: worker_online
      # Reset worker node c≈© (n·∫øu c√≥)

    - name: Execute join command
      shell: "{{ join_script }}"
      register: join_output
      ignore_errors: yes
      when: worker_online
      # Th·ª±c thi l·ªánh join ƒë·ªÉ worker tham gia cluster

    - name: Display join result summary
      debug:
        msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"
      # B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng

- name: Step 7 - Verify Kubernetes cluster status
  hosts: master
  become: yes
  gather_facts: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Check kubectl binary
      command: which kubectl
      register: kubectl_check
      failed_when: kubectl_check.rc != 0
      changed_when: false
      # Ki·ªÉm tra kubectl ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t

    - name: List all nodes
      command: kubectl get nodes -o wide
      register: nodes_info
      changed_when: false
      # Li·ªát k√™ t·∫•t c·∫£ c√°c node trong cluster

    - name: List system pods
      command: kubectl get pods -n kube-system -o wide
      register: pods_info
      changed_when: false
      # Li·ªát k√™ c√°c pod trong namespace kube-system

    - name: Display cluster info
      debug:
        msg:
          - "Node list:"
          - "{{ nodes_info.stdout_lines }}"
          - "Pods in kube-system namespace:"
          - "{{ pods_info.stdout_lines }}"
      # Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt v·ªÅ cluster

    - name: Check node readiness
      shell: kubectl get nodes --no-headers | awk '{print $1, $2}' | column -t
      register: node_status
      changed_when: false
      # Ki·ªÉm tra tr·∫°ng th√°i Ready c·ªßa c√°c node

    - name: Node status summary
      debug:
        msg: |
          {% if 'NotReady' in node_status.stdout %}
          Some nodes are not ready:
          {{ node_status.stdout }}
          Please check kubelet or CNI (Calico) on those nodes.
          {% else %}
          All nodes are in Ready state!
          {{ node_status.stdout }}
          {% endif %}
      # T·ªïng k·∫øt tr·∫°ng th√°i node

    - name: Detect problematic pods
      shell: kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' || true
      register: bad_pods
      changed_when: false
      # Ph√°t hi·ªán c√°c pod c√≥ v·∫•n ƒë·ªÅ

    - name: Report problematic pods
      debug:
        msg: |
          {% if bad_pods.stdout %}
          Some pods in kube-system are not stable:
          {{ bad_pods.stdout }}
          {% else %}
          All kube-system pods are Running or Completed!
          {% endif %}
      # B√°o c√°o c√°c pod c√≥ v·∫•n ƒë·ªÅ

    - name: Collect logs from problematic pods
      when: bad_pods.stdout != ""
      shell: |
        echo "==== Problematic Pod Logs ===="
        for pod in $(kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' | awk '{print $1}'); do
          echo "---------------------------------------------"
          echo "Pod: $pod"
          kubectl logs -n kube-system $pod --tail=30 || echo "Cannot get logs for $pod"
          echo
        done
      register: bad_pods_logs
      ignore_errors: yes
      # Thu th·∫≠p log c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ

    - name: Display detailed logs
      when: bad_pods.stdout != ""
      debug:
        msg: "{{ bad_pods_logs.stdout_lines | default(['No error logs or pods restarted.']) }}"
      # Hi·ªÉn th·ªã log chi ti·∫øt c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ`
    ,

    'deploy-full-cluster-flannel': `---
- name: Step 0 - Reset Kubernetes cluster
  hosts: all
  become: yes
  gather_facts: yes
  environment:
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Reset old cluster if exists
      shell: kubeadm reset -f || true
      ignore_errors: true
      # Reset c·ª•m Kubernetes c≈© n·∫øu t·ªìn t·∫°i

    - name: Remove old Kubernetes configuration
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes
        - /etc/cni/net.d
        - /root/.kube
        - "/home/{{ ansible_user }}/.kube"
      ignore_errors: true
      # X√≥a c√°c th∆∞ m·ª•c c·∫•u h√¨nh Kubernetes c≈©

    - name: Restart containerd service
      shell: systemctl restart containerd || true
      ignore_errors: true
      # Kh·ªüi ƒë·ªông l·∫°i containerd ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch s·∫Ω

- name: Step 1 - Update /etc/hosts and hostname
  hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Update /etc/hosts file for all nodes
      lineinfile:
        path: /etc/hosts
        line: "{{ hostvars[item].ansible_host }} {{ item }}"
        state: present
        create: yes
        insertafter: EOF
      loop: "{{ groups['all'] }}"
      when: hostvars[item].ansible_host is defined
      # Th√™m danh s√°ch c√°c node v√†o /etc/hosts ƒë·ªÉ cluster nh·∫≠n di·ªán nhau

    - name: Set hostname according to inventory
      hostname:
        name: "{{ inventory_hostname }}"
      when: ansible_hostname != inventory_hostname
      # ƒê·∫∑t hostname theo t√™n trong inventory

    - name: Verify hostname
      shell: hostnamectl
      register: host_info
      # Ki·ªÉm tra hostname ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t ƒë√∫ng

    - name: Display hostname info
      debug:
        msg: "{{ host_info.stdout_lines }}"
      # Hi·ªÉn th·ªã th√¥ng tin hostname

- name: Step 2 - Configure kernel and containerd
  hosts: all
  become: yes
  gather_facts: no
  tasks:
    - name: Disable swap
      shell: swapoff -a && sed -i '/swap/d' /etc/fstab || true
      # T·∫Øt swap v√¨ Kubernetes kh√¥ng h·ªó tr·ª£

    - name: Load kernel modules for containerd
      copy:
        dest: /etc/modules-load.d/containerd.conf
        content: |
          overlay
          br_netfilter
      # T·∫£i c√°c kernel module c·∫ßn thi·∫øt cho containerd

    - name: Activate kernel modules
      shell: |
        modprobe overlay || true
        modprobe br_netfilter || true
      # K√≠ch ho·∫°t module overlay v√† br_netfilter

    - name: Configure sysctl parameters for Kubernetes
      copy:
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
      # C·∫•u h√¨nh sysctl cho networking Kubernetes

    - name: Apply sysctl configuration
      command: sysctl --system
      # √Åp d·ª•ng c·∫•u h√¨nh sysctl

    - name: Install containerd runtime
      apt:
        name: containerd
        state: present
        update_cache: yes
      # C√†i ƒë·∫∑t containerd container runtime

    - name: Generate default containerd config
      shell: |
        mkdir -p /etc/containerd
        containerd config default > /etc/containerd/config.toml
      # T·∫°o file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd

    - name: Enable SystemdCgroup
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'
      # B·∫≠t SystemdCgroup trong containerd

    - name: Restart and enable containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes
      # Kh·ªüi ƒë·ªông l·∫°i v√† b·∫≠t containerd ƒë·ªÉ √°p d·ª•ng c·∫•u h√¨nh

- name: Step 3 - Install Kubernetes core components
  hosts: all
  become: yes
  gather_facts: no
  tasks:
    - name: Add Kubernetes GPG key
      shell: |
        mkdir -p /usr/share/keyrings
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
        gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
      changed_when: false
      ignore_errors: true
      # Th√™m GPG key ch√≠nh th·ª©c c·ªßa Kubernetes

    - name: Add Kubernetes repository
      copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        content: |
          deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
      # Th√™m repository APT c·ªßa Kubernetes

    - name: Install kubelet, kubeadm, kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes
      # C√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn core c·ªßa Kubernetes

    - name: Hold package version
      command: apt-mark hold kubelet kubeadm kubectl
      # Gi·ªØ phi√™n b·∫£n c√°c package ƒë·ªÉ tr√°nh c·∫≠p nh·∫≠t t·ª± ƒë·ªông

- name: Step 4 - Initialize Master node
  hosts: master
  become: yes
  gather_facts: yes
  tasks:
    - name: Get master IP address
      set_fact:
        master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
      # L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa master node

    - name: Reset old control plane (if any)
      shell: kubeadm reset -f || true
      # Reset control plane c≈© n·∫øu c√≥

    - name: Initialize Kubernetes control plane
      command: >
        kubeadm init
        --control-plane-endpoint "{{ master_ip }}:6443"
        --apiserver-advertise-address {{ master_ip }}
        --pod-network-cidr 10.244.0.0/16
      args:
        creates: /etc/kubernetes/admin.conf
      register: kubeadm_init
      failed_when: "'error' in kubeadm_init.stderr"
      changed_when: "'Your Kubernetes control-plane has initialized successfully' in kubeadm_init.stdout"
      # Kh·ªüi t·∫°o control plane Kubernetes v·ªõi Flannel CNI

    - name: Copy kubeconfig for root
      shell: |
        mkdir -p /root/.kube
        cp -i /etc/kubernetes/admin.conf /root/.kube/config
        chown root:root /root/.kube/config
      args:
        executable: /bin/bash
      # Sao ch√©p kubeconfig cho user root

    - name: Copy kubeconfig for normal user
      when: ansible_user != "root"
      block:
        - name: Create ~/.kube directory
          file:
            path: "/home/{{ ansible_user }}/.kube"
            state: directory
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0755'
          # T·∫°o th∆∞ m·ª•c kubeconfig cho user th∆∞·ªùng

        - name: Copy kubeconfig file
          copy:
            src: /etc/kubernetes/admin.conf
            dest: "/home/{{ ansible_user }}/.kube/config"
            remote_src: yes
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0600'
          # Sao ch√©p kubeconfig cho user th∆∞·ªùng
      # Sao ch√©p kubeconfig cho user th∆∞·ªùng (n·∫øu kh√¥ng ph·∫£i root)

- name: Step 5 - Install Flannel CNI
  hosts: master
  become: yes
  gather_facts: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    DEBIAN_FRONTEND: noninteractive
  tasks:
    - name: Apply Flannel CNI manifest
      command: kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
      register: flannel_apply
      changed_when: "'created' in flannel_apply.stdout or 'configured' in flannel_apply.stdout"
      # √Åp d·ª•ng manifest Flannel CNI

    - name: Wait for Flannel pods to be Running
      shell: |
        kubectl get pods -n kube-flannel --no-headers 2>/dev/null | grep -c 'Running' || true
      register: flannel_running
      retries: 10
      delay: 15
      until: flannel_running.stdout | int > 0
      # Ch·ªù c√°c pod Flannel kh·ªüi ƒë·ªông v√† ·ªü tr·∫°ng th√°i Running

    - name: Confirm Flannel pods are active
      debug:
        msg: "Flannel is running ({{ flannel_running.stdout }} pods Running)."
      # X√°c nh·∫≠n Flannel ƒëang ho·∫°t ƒë·ªông

- name: Step 6 - Join worker nodes
  hosts: workers
  become: yes
  gather_facts: false
  vars:
    join_script: /tmp/kube_join.sh
  tasks:
    - name: Test SSH connectivity to worker node
      ping:
      register: ping_result
      ignore_errors: yes
      # Ki·ªÉm tra k·∫øt n·ªëi SSH ƒë·∫øn worker node

    - name: Mark worker online status
      set_fact:
        worker_online: "{{ ping_result is succeeded }}"
      # ƒê√°nh d·∫•u worker n√†o online/offline

    - name: Display worker online status
      debug:
        msg: "Worker {{ inventory_hostname }} is {{ 'ONLINE' if worker_online else 'OFFLINE' }}"
      # Hi·ªÉn th·ªã tr·∫°ng th√°i online/offline

    - name: Retrieve join command from master
      delegate_to: "{{ groups['master'][0] }}"
      run_once: true
      shell: kubeadm token create --print-join-command
      register: join_cmd
      when: worker_online
      # L·∫•y l·ªánh join t·ª´ master node (ch·ªâ ch·∫°y 1 l·∫ßn)

    - name: Save join command to script file
      copy:
        content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
        dest: "{{ join_script }}"
        mode: '0755'
      when: worker_online
      ignore_errors: yes
      # L∆∞u l·ªánh join v√†o file script

    - name: Reset old worker node
      shell: kubeadm reset -f || true
      ignore_errors: yes
      when: worker_online
      # Reset worker node c≈© (n·∫øu c√≥)

    - name: Execute join command
      shell: "{{ join_script }}"
      register: join_output
      ignore_errors: yes
      when: worker_online
      # Th·ª±c thi l·ªánh join ƒë·ªÉ worker tham gia cluster

    - name: Display join result summary
      debug:
        msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"
      # B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng

- name: Step 7 - Verify Kubernetes cluster status
  hosts: master
  become: yes
  gather_facts: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  tasks:
    - name: Check kubectl binary
      command: which kubectl
      register: kubectl_check
      failed_when: kubectl_check.rc != 0
      changed_when: false
      # Ki·ªÉm tra kubectl ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t

    - name: List all nodes
      command: kubectl get nodes -o wide
      register: nodes_info
      changed_when: false
      # Li·ªát k√™ t·∫•t c·∫£ c√°c node trong cluster

    - name: List system pods
      command: kubectl get pods -n kube-system -o wide
      register: pods_info
      changed_when: false
      # Li·ªát k√™ c√°c pod trong namespace kube-system

    - name: Display cluster info
      debug:
        msg:
          - "Node list:"
          - "{{ nodes_info.stdout_lines }}"
          - "Pods in kube-system namespace:"
          - "{{ pods_info.stdout_lines }}"
      # Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt v·ªÅ cluster

    - name: Check node readiness
      shell: kubectl get nodes --no-headers | awk '{print $1, $2}' | column -t
      register: node_status
      changed_when: false
      # Ki·ªÉm tra tr·∫°ng th√°i Ready c·ªßa c√°c node

    - name: Node status summary
      debug:
        msg: |
          {% if 'NotReady' in node_status.stdout %}
          Some nodes are not ready:
          {{ node_status.stdout }}
          Please check kubelet or CNI (Flannel) on those nodes.
          {% else %}
          All nodes are in Ready state!
          {{ node_status.stdout }}
          {% endif %}
      # T·ªïng k·∫øt tr·∫°ng th√°i node

    - name: Detect problematic pods
      shell: kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' || true
      register: bad_pods
      changed_when: false
      # Ph√°t hi·ªán c√°c pod c√≥ v·∫•n ƒë·ªÅ

    - name: Report problematic pods
      debug:
        msg: |
          {% if bad_pods.stdout %}
          Some pods in kube-system are not stable:
          {{ bad_pods.stdout }}
          {% else %}
          All kube-system pods are Running or Completed!
          {% endif %}
      # B√°o c√°o c√°c pod c√≥ v·∫•n ƒë·ªÅ

    - name: Collect logs from problematic pods
      when: bad_pods.stdout != ""
      shell: |
        echo "==== Problematic Pod Logs ===="
        for pod in $(kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' | awk '{print $1}'); do
          echo "---------------------------------------------"
          echo "Pod: $pod"
          kubectl logs -n kube-system $pod --tail=30 || echo "Cannot get logs for $pod"
          echo
        done
      register: bad_pods_logs
      ignore_errors: yes
      # Thu th·∫≠p log c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ

    - name: Display detailed logs
      when: bad_pods.stdout != ""
      debug:
        msg: "{{ bad_pods_logs.stdout_lines | default(['No error logs or pods restarted.']) }}"
      # Hi·ªÉn th·ªã log chi ti·∫øt c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ
`
  };

  // Map template value (without numbers) to template key (with numbers)
  const templateMapping = {
    'update-hosts-hostname': '01-update-hosts-hostname',
    'kernel-sysctl': '02-kernel-sysctl',
    'install-containerd': '03-install-containerd',
    'install-kubernetes': '04-install-kubernetes',
    'init-master': '05-init-master',
    'install-cni': '06-install-cni',
    'install-flannel': '06-install-flannel',
    'join-workers': '07-join-workers',
    'verify-cluster': '08-verify-cluster',
    'install-ingress': '09-install-ingress',
    'install-helm': '10-install-helm',
    'setup-storage': '11-setup-storage',
    'prepare-and-join-worker': '12-prepare-and-join-worker',
    'deploy-full-cluster': 'deploy-full-cluster',
    'deploy-full-cluster-flannel': 'deploy-full-cluster-flannel',
    'reset-cluster': '00-reset-cluster'
  };

  // Get the actual template key
  const actualTemplate = templateMapping[template] || template;

  const playbookContent = templates[actualTemplate];
  if (!playbookContent) {
    throw new Error('Template kh√¥ng t·ªìn t·∫°i');
  }

  const filename = actualTemplate + '.yml';

  // Check if playbook already exists
  const exists = await checkPlaybookExists(filename);
  console.log(`Checking if ${filename} exists:`, exists);

  if (exists) {
    const templateNames = {
      '00-reset-cluster': 'Reset to√†n b·ªô cluster',
      '01-update-hosts-hostname': 'C·∫≠p nh·∫≠t hosts & hostname',
      '02-kernel-sysctl': 'C·∫•u h√¨nh kernel & sysctl',
      '03-install-containerd': 'C√†i ƒë·∫∑t Containerd',
      '04-install-kubernetes': 'C√†i ƒë·∫∑t Kubernetes',
      '05-init-master': 'Kh·ªüi t·∫°o Master',
      '06-install-cni': 'C√†i CNI (Calico)',
      '06-install-flannel': 'C√†i CNI (Flannel)',
      '07-join-workers': 'Join Workers',
      '08-verify-cluster': 'X√°c minh tr·∫°ng th√°i c·ª•m',
      '09-install-ingress': 'C√†i Ingress Controller',
      '10-install-helm': 'C√†i Helm',
      '11-setup-storage': 'Setup Storage',
      '12-prepare-and-join-worker': 'Chu·∫©n b·ªã & Join Worker (02‚Üí03‚Üí04‚Üí07)',
      'deploy-full-cluster': 'Tri·ªÉn khai to√†n b·ªô cluster (Calico)',
      'deploy-full-cluster-flannel': 'Tri·ªÉn khai to√†n b·ªô cluster (Flannel)'
    };

    const templateName = templateNames[actualTemplate] || actualTemplate;
    const confirmMessage = `Playbook "${filename}" (${templateName}) ƒë√£ t·ªìn t·∫°i. B·∫°n c√≥ mu·ªën ghi ƒë√® l√™n file c≈©?`;

    console.log('Showing confirm dialog for:', confirmMessage);

    if (!confirm(confirmMessage)) {
      console.log('User cancelled overwrite');
      throw new Error('ƒê√£ h·ªßy t·∫°o playbook t·ª´ template');
    }

    console.log('User confirmed overwrite');
  }

  // Thay th·∫ø cluster_id trong n·ªôi dung playbook n·∫øu c√≥
  let finalPlaybookContent = playbookContent;
  if (getClusterId() && playbookContent.includes('{{ cluster_id | default(1) }}')) {
    finalPlaybookContent = playbookContent.replace('{{ cluster_id | default(1) }}', getClusterId());
  }

  try {
    const result = await fetch(`/api/ansible-playbook/save/${getClusterId()}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
      },
      body: `filename=${encodeURIComponent(filename)}&content=${encodeURIComponent(finalPlaybookContent)}`
    });

    if (!result.ok) {
      const errorData = await result.json();
      throw new Error(errorData.error || 'L·ªói t·∫°o playbook');
    }

    return await result.json();
  } catch (error) {
    console.error('L·ªói t·∫°o playbook K8s:', error);
    throw error;
  }
}

// Hi·ªÉn th·ªã n·ªôi dung playbook
function showPlaybookContentView() {
  const contentArea = document.getElementById('playbook-content-area');
  const executionArea = document.getElementById('playbook-execution-status');

  if (contentArea) contentArea.style.display = 'block';
  if (executionArea) executionArea.style.display = 'none';
}

// Hi·ªÉn th·ªã th·ª±c thi playbook
function showPlaybookExecutionView() {
  const contentArea = document.getElementById('playbook-content-area');
  const executionArea = document.getElementById('playbook-execution-status');

  if (contentArea) contentArea.style.display = 'none';
  if (executionArea) executionArea.style.display = 'block';
}

// T√¨m ki·∫øm playbook
function searchPlaybooks(query) {
  const items = document.querySelectorAll('#playbook-list .playbook-item');
  const searchTerm = query.toLowerCase().trim();

  items.forEach(item => {
    const playbookName = item.textContent.toLowerCase();
    const shouldShow = playbookName.includes(searchTerm);

    const listItem = item.closest('.list-group-item');
    if (listItem) {
      listItem.style.display = shouldShow ? 'flex' : 'none';
    }
  });
}

// C·∫≠p nh·∫≠t danh s√°ch playbook
window.refreshPlaybooks = async function () {
  const refreshBtn = document.getElementById('refresh-playbooks-btn');
  if (refreshBtn) {
    refreshBtn.disabled = true;
    refreshBtn.innerHTML = '<span class="spinner-border spinner-border-sm me-2"></span>ƒêang t·∫£i...';
  }

  try {
    await loadPlaybooks();
  } catch (error) {
    console.error('Error refreshing playbooks:', error);
    showAlert('error', 'L·ªói l√†m m·ªõi danh s√°ch playbook');
  } finally {
    if (refreshBtn) {
      refreshBtn.disabled = false;
      refreshBtn.innerHTML = 'L√†m m·ªõi';
    }
  }
};

// ƒê·∫∑t ID cluster hi·ªán t·∫°i
function setCurrentClusterId(clusterId) {
  currentClusterId = clusterId;
}

// Xu·∫•t c√°c h√†m cho truy c·∫≠p to√†n c·ª•c
window.loadPlaybook = loadPlaybook;
window.savePlaybook = savePlaybook;
window.deletePlaybook = deletePlaybook;
window.executePlaybook = executePlaybook;
window.uploadPlaybook = uploadPlaybook;
window.refreshPlaybooks = refreshPlaybooks;
window.searchPlaybooks = searchPlaybooks;
window.showPlaybookContentView = showPlaybookContentView;
window.showPlaybookExecutionView = showPlaybookExecutionView;
window.setCurrentClusterId = setCurrentClusterId;
window.generateK8sPlaybookFromTemplate = generateK8sPlaybookFromTemplate;

// Reset giao di·ªán Playbook Manager khi r·ªùi kh·ªèi trang chi ti·∫øt cluster
window.resetPlaybookUI = function () {
  try {
    const list = document.getElementById('playbook-list');
    if (list) list.innerHTML = '';
    const fn = document.getElementById('playbook-filename');
    if (fn) fn.value = '';
    const ed = document.getElementById('playbook-editor');
    if (ed) ed.value = '';
    const contentArea = document.getElementById('playbook-content-area');
    const execArea = document.getElementById('playbook-execution-status');
    if (contentArea) contentArea.style.display = 'none';
    if (execArea) execArea.style.display = 'none';
    const output = document.getElementById('ansible-output');
    if (output) output.textContent = '';
    const alertBox = document.getElementById('playbook-alert');
    if (alertBox) alertBox.innerHTML = '';
    const search = document.getElementById('playbook-search');
    if (search) search.value = '';
  } catch (_) { }
}
