// Playbook Manager - Qu·∫£n l√Ω playbook v√† template K8s

function getClusterId() {
	let cid = window.currentClusterId;
	if (!cid) {
	  try {
		const params = new URLSearchParams(window.location.search || '');
		const fromUrl = params.get('clusterId');
		if (fromUrl) {
		  window.currentClusterId = fromUrl;
		  cid = fromUrl;
		}
	  } catch (_) { }
	}
	return cid;
  }
  // Load playbook cho cluster hi·ªán t·∫°i (optional override)
  async function loadPlaybooks(clusterIdOverride) {
	const cid = clusterIdOverride || getClusterId();
	if (!cid) {
	  console.error('No cluster selected');
	  return;
	}
  
	try {
	  // L∆∞u l·∫°i override n·∫øu c√≥
	  if (clusterIdOverride) {
		window.currentClusterId = clusterIdOverride;
	  }
	  const response = await fetch(`/api/ansible-playbook/list/${cid}`);
	  if (!response.ok) {
		throw new Error('Failed to load playbooks');
	  }
  
	  const playbooks = await response.json();
	  const playbookList = document.getElementById('playbook-list');
  
	  if (playbooks.length === 0) {
		playbookList.innerHTML = '<div class="list-group-item text-center text-muted">Ch∆∞a c√≥ playbook n√†o</div>';
	  } else {
		playbookList.innerHTML = '';
		playbooks.forEach(pb => {
		  const item = document.createElement('div');
		  item.className = 'list-group-item d-flex justify-content-between align-items-center';
		  item.innerHTML = `
			<div class="playbook-item" data-name="${pb}">
			  <div class="fw-bold">${pb}</div>
			  <div class="small text-muted">Playbook file</div>
			</div>
			<div class="btn-group btn-group-sm">
			  <button class="btn btn-outline-primary btn-sm" onclick="loadPlaybook('${pb}')" title="Xem">
				üëÅÔ∏è
			  </button>
			  <button class="btn btn-outline-success btn-sm" onclick="executePlaybook('${pb}')" title="Th·ª±c thi">
				‚ñ∂Ô∏è
			  </button>
			  <button class="btn btn-outline-danger btn-sm" onclick="deletePlaybook('${pb}')" title="X√≥a">
				üóëÔ∏è
			  </button>
			</div>
		  `;
		  playbookList.appendChild(item);
		});
	  }
	} catch (error) {
	  console.error('Error loading playbooks:', error);
	  // Hi·ªÉn th·ªã l·ªói trong playbook list thay v√¨ d√πng showAlert
	  const playbookList = document.getElementById('playbook-list');
	  if (playbookList) {
		playbookList.innerHTML = `
		  <div class="list-group-item text-center text-danger">
			<i class="bi bi-exclamation-triangle me-2"></i>
			L·ªói t·∫£i danh s√°ch playbook: ${error.message}
		  </div>
		`;
	  }
	}
  }
  
  // T·∫£i n·ªôi dung playbook
  window.loadPlaybook = async function (filename) {
	const cid = getClusterId();
	if (!cid || !filename) return;
  
	try {
	  const response = await fetch(`/api/ansible-playbook/read/${cid}/${filename}`);
	  if (!response.ok) {
		throw new Error('Failed to load playbook');
	  }
  
	  const data = await response.json();
	  document.getElementById('playbook-filename').value = filename.replace('.yml', '');
	  document.getElementById('playbook-editor').value = data.content;
  
	  // Hi·ªÉn th·ªã view n·ªôi dung v√† ·∫©n view th·ª±c thi
	  showPlaybookContentView();
  
	} catch (error) {
	  console.error('Error loading playbook:', error);
	  showAlert('error', 'L·ªói t·∫£i playbook: ' + error.message);
	}
  };
  
  // L∆∞u playbook
  window.savePlaybook = async function () {
	const cid = getClusterId();
	if (!cid) return;
  
	const filename = document.getElementById('playbook-filename')?.value;
	const content = document.getElementById('playbook-editor')?.value;
  
	if (!filename || !content) {
	  showAlert('error', 'Vui l√≤ng nh·∫≠p t√™n file v√† n·ªôi dung');
	  return;
	}
  
	try {
	  const response = await fetch(`/api/ansible-playbook/save/${cid}`, {
		method: 'POST',
		headers: {
		  'Content-Type': 'application/x-www-form-urlencoded',
		},
		body: `filename=${encodeURIComponent(filename)}&content=${encodeURIComponent(content)}`
	  });
  
	  if (!response.ok) {
		const errorData = await response.json();
		throw new Error(errorData.error || 'L·ªói l∆∞u playbook');
	  }
  
	  showAlert('success', 'ƒê√£ l∆∞u playbook th√†nh c√¥ng');
	  await loadPlaybooks(); // C·∫≠p nh·∫≠t danh s√°ch
  
	} catch (error) {
	  console.error('Error saving playbook:', error);
	  showAlert('error', 'L·ªói l∆∞u playbook: ' + error.message);
	}
  };
  
  // X√≥a playbook
  window.deletePlaybook = async function (filename) {
	const cid = getClusterId();
	if (!cid || !filename) return;
  
	if (!confirm(`X√≥a playbook "${filename}"?`)) return;
  
	try {
	  const response = await fetch(`/api/ansible-playbook/delete/${cid}/${filename}`, {
		method: 'DELETE'
	  });
  
	  if (!response.ok) {
		const errorData = await response.json();
		throw new Error(errorData.error || 'L·ªói x√≥a playbook');
	  }
  
	  showAlert('success', `ƒê√£ x√≥a playbook "${filename}" th√†nh c√¥ng `);
	  await loadPlaybooks(); // C·∫≠p nh·∫≠t danh s√°ch
  
	} catch (error) {
	  console.error('Error deleting playbook:', error);
	  showAlert('error', 'L·ªói x√≥a playbook: ' + error.message);
	}
  };
  
  // Th·ª±c thi playbook
  window.executePlaybook = async function (filename, extraVars = '') {
	const cid = getClusterId();
	if (!cid || !filename) return;
  
	try {
	  // Hi·ªÉn th·ªã th·ª±c thi v√† ·∫©n n·ªôi dung
	  showPlaybookExecutionView();
  
	  const response = await fetch(`/api/ansible-playbook/execute/${cid}`, {
		method: 'POST',
		headers: {
		  'Content-Type': 'application/x-www-form-urlencoded',
		},
		body: `filename=${encodeURIComponent(filename)}&extraVars=${encodeURIComponent(extraVars)}`
	  });
  
	  if (!response.ok) {
		const errorData = await response.json();
		throw new Error(errorData.error || 'L·ªói th·ª±c thi playbook');
	  }
  
	  const result = await response.json();
	  showAlert('success', `ƒê√£ b·∫Øt ƒë·∫ßu th·ª±c thi playbook: ${filename}`);
  
	  // B·∫Øt ƒë·∫ßu theo d√µi tr·∫°ng th√°i th·ª±c thi
	  if (result.taskId) {
		monitorPlaybookExecution(result.taskId);
	  }
  
	  return result;
	} catch (error) {
	  console.error('Error executing playbook:', error);
	  showAlert('error', 'L·ªói th·ª±c thi playbook: ' + error.message);
	  showPlaybookContentView(); // Hi·ªÉn th·ªã n·ªôi dung khi c√≥ l·ªói
	  throw error;
	}
  };
  
  // Theo d√µi th·ª±c thi playbook
  async function monitorPlaybookExecution(taskId) {
	const outputElement = document.getElementById('ansible-output');
	const progressElement = document.getElementById('execution-progress');
	const spinnerElement = document.getElementById('execution-spinner');
  
	if (!outputElement || !progressElement || !spinnerElement) {
	  console.error('Execution elements not found');
	  return;
	}
  
	// X√≥a output tr∆∞·ªõc
	outputElement.innerHTML = '';
  
	// Hi·ªÉn th·ªã progress v√† spinner
	progressElement.style.display = 'block';
	spinnerElement.style.display = 'inline-block';
  
	const checkStatus = async () => {
	  try {
		const response = await fetch(`/api/ansible-playbook/status/${getClusterId()}/${taskId}`);
		if (!response.ok) {
		  throw new Error('Failed to check status');
		}
  
		const status = await response.json();
  
		// C·∫≠p nh·∫≠t progress bar
		const progressBar = progressElement.querySelector('.progress-bar');
		if (progressBar) {
		  progressBar.style.width = `${status.progress || 0}%`;
		  progressBar.setAttribute('aria-valuenow', status.progress || 0);
		}
  
		// C·∫≠p nh·∫≠t output
		if (status.output && status.output.length > 0) {
		  const newOutput = status.output.slice(outputElement.children.length);
		  newOutput.forEach(line => {
			const lineElement = document.createElement('div');
			lineElement.className = 'output-line';
  
			// M√£ h√≥a m√†u cho c√°c lo·∫°i output kh√°c nhau
			if (line.includes('TASK') || line.includes('PLAY')) {
			  lineElement.className += ' task-header';
			} else if (line.includes('ok:') || line.includes('changed:')) {
			  lineElement.className += ' success';
			} else if (line.includes('fatal:') || line.includes('failed:')) {
			  lineElement.className += ' error';
			} else if (line.includes('skipping:')) {
			  lineElement.className += ' warning';
			}
  
			lineElement.textContent = line;
			outputElement.appendChild(lineElement);
		  });
  
		  // Cu·ªôn xu·ªëng cu·ªëi
		  outputElement.scrollTop = outputElement.scrollHeight;
		}
  
		if (status.status === 'running') {
		  setTimeout(checkStatus, 1000);
		} else {
		  // ·∫®n spinner
		  spinnerElement.style.display = 'none';
  
		  // Hi·ªÉn th·ªã th√¥ng b√°o ho√†n th√†nh
		  const summaryElement = document.createElement('div');
		  summaryElement.className = 'text-success mt-3 border-top pt-2';
  
		  const titleElement = document.createElement('div');
		  titleElement.className = 'fw-bold';
		  titleElement.textContent = 'Ho√†n th√†nh th·ª±c thi playbook!';
  
		  const timeElement = document.createElement('div');
		  timeElement.className = 'small text-white';
		  timeElement.textContent = `Th·ªùi gian th·ª±c thi: ${Math.round((status.endTime - status.startTime) / 1000)}s`;
  
		  summaryElement.appendChild(titleElement);
		  summaryElement.appendChild(timeElement);
		  outputElement.appendChild(summaryElement);
		}
  
	  } catch (error) {
		console.error('L·ªói theo d√µi th·ª±c thi:', error);
		spinnerElement.style.display = 'none';
  
		const errorElement = document.createElement('div');
		errorElement.className = 'text-danger mt-3';
  
		const errorTitle = document.createElement('div');
		errorTitle.className = 'fw-bold';
		errorTitle.textContent = 'L·ªói ki·ªÉm tra tr·∫°ng th√°i th·ª±c thi';
  
		errorElement.appendChild(errorTitle);
		outputElement.appendChild(errorElement);
	  }
	};
  
	checkStatus();
  }
  
  // Ki·ªÉm tra xem playbook c√≥ t·ªìn t·∫°i kh√¥ng
  async function checkPlaybookExists(filename) {
	const cid = getClusterId();
	if (!cid) {
	  console.log('Kh√¥ng t√¨m th·∫•y ID cluster');
	  return false;
	}
  
	try {
	  const response = await fetch(`/api/ansible-playbook/list/${cid}`);
	  if (!response.ok) {
		console.log('L·ªói t·∫£i danh s√°ch playbook:', response.status);
		return false;
	  }
  
	  const playbooks = await response.json();
	  console.log('Playbook hi·ªán t·∫°i:', playbooks);
	  console.log('T√¨m ki·∫øm:', filename);
  
	  const exists = playbooks.includes(filename);
	  console.log('File t·ªìn t·∫°i:', exists);
	  return exists;
	} catch (error) {
	  console.error('L·ªói ki·ªÉm tra playbook:', error);
	  return false;
	}
  }
  
  // T·∫£i l√™n playbook
  window.uploadPlaybook = async function (file) {
	const cid = getClusterId();
	if (!cid || !file) return;
  
	try {
	  // Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
	  const originalFilename = file.name;
	  const finalFilename = originalFilename.toLowerCase().endsWith('.yml') || originalFilename.toLowerCase().endsWith('.yaml')
		? originalFilename
		: originalFilename + '.yml';
  
	  const exists = await checkPlaybookExists(finalFilename);
  
	  if (exists) {
		const confirmMessage = `Playbook "${finalFilename}" ƒë√£ t·ªìn t·∫°i. B·∫°n c√≥ mu·ªën ghi ƒë√® l√™n file c≈© kh√¥ng?`;
		if (!confirm(confirmMessage)) {
		  showAlert('info', 'ƒê√£ h·ªßy t·∫£i l√™n playbook');
		  return;
		}
	  }
  
	  const formData = new FormData();
	  formData.append('file', file);
  
	  const response = await fetch(`/api/ansible-playbook/upload/${cid}`, {
		method: 'POST',
		body: formData
	  });
  
	  if (!response.ok) {
		const errorData = await response.json();
		throw new Error(errorData.error || 'L·ªói t·∫£i l√™n playbook');
	  }
  
	  const result = await response.json();
	  showAlert('success', `ƒê√£ t·∫£i l√™n playbook: ${result.filename}`);
	  await loadPlaybooks(); // C·∫≠p nh·∫≠t danh s√°ch
  
	  // T·∫£i n·ªôi dung playbook ƒë√£ t·∫£i l√™n
	  await loadPlaybook(result.filename);
  
	} catch (error) {
	  console.error('L·ªói t·∫£i l√™n playbook:', error);
	  showAlert('error', 'L·ªói t·∫£i l√™n playbook: ' + error.message);
	  throw error;
	}
  };
  
  // T·∫°o playbook K8s t·ª´ template
  async function generateK8sPlaybookFromTemplate(template) {
	if (!getClusterId()) {
	  throw new Error('Vui l√≤ng ch·ªçn cluster tr∆∞·ªõc');
	}
  
	const templates = {
	  '01-update-hosts-hostname': `---
  - name: Update /etc/hosts and hostname for entire cluster
	hosts: all
	become: yes
	gather_facts: yes
	tasks:
	  - name: Add all inventory nodes to /etc/hosts
		lineinfile:
		  path: /etc/hosts
		  line: "{{ hostvars[item].ansible_host | default(item) }} {{ hostvars[item].ansible_user }}"
		  state: present
		  create: yes
		  insertafter: EOF
		loop: "{{ groups['all'] }}"
		when: hostvars[item].ansible_user is defined
		tags: addhosts
		# Th√™m t·∫•t c·∫£ node trong inventory v√†o /etc/hosts
		
	  - name: Set hostname according to inventory
		hostname:
		  name: "{{ hostvars[inventory_hostname].ansible_user }}"
		when: ansible_hostname != hostvars[inventory_hostname].ansible_user
		tags: sethostname
		# ƒê·∫∑t hostname theo inventory
		
	  - name: Verify hostname after update
		command: hostnamectl
		register: host_info
		changed_when: false
		tags: verify
		# Ki·ªÉm tra hostname sau khi c·∫≠p nh·∫≠t
		
	  - name: Display information after update
		debug:
		  msg:
			- "Hostname hi·ªán t·∫°i: {{ ansible_hostname }}"
			- "K·∫øt qu·∫£ l·ªánh hostnamectl:"
			- "{{ host_info.stdout_lines }}"
		tags: verify
		# Hi·ªÉn th·ªã th√¥ng tin sau khi c·∫≠p nh·∫≠t`,
  
	  '02-kernel-sysctl': `---
  - hosts: all
	become: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Disable swap
		shell: swapoff -a || true
		ignore_errors: true
		# T·∫Øt swap v√¨ Kubernetes kh√¥ng h·ªó tr·ª£
  
	  - name: Comment swap lines in /etc/fstab
		replace:
		  path: /etc/fstab
		  regexp: '(^.*swap.*$)'
		  replace: '# \\1'
		# Comment d√≤ng swap trong /etc/fstab
  
	  - name: Load kernel modules
		copy:
		  dest: /etc/modules-load.d/containerd.conf
		  content: |
			overlay
			br_netfilter
		# T·∫£i module kernel cho containerd
  
	  - name: Load overlay and br_netfilter modules
		shell: |
		  modprobe overlay
		  modprobe br_netfilter
		# K√≠ch ho·∫°t module overlay v√† br_netfilter
  
	  - name: Configure sysctl for Kubernetes
		copy:
		  dest: /etc/sysctl.d/99-kubernetes-cri.conf
		  content: |
			net.bridge.bridge-nf-call-iptables  = 1
			net.bridge.bridge-nf-call-ip6tables = 1
			net.ipv4.ip_forward                 = 1
		# C·∫•u h√¨nh sysctl cho Kubernetes
  
	  - name: Apply sysctl configuration
		command: sysctl --system
		# √Åp d·ª•ng c·∫•u h√¨nh sysctl`,
  
	  '03-install-containerd': `---
  - hosts: all
	become: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Update apt cache
		apt:
		  update_cache: yes
		# C·∫≠p nh·∫≠t cache APT
  
	  - name: Install containerd
		apt:
		  name: containerd
		  state: present
		  force_apt_get: yes
		# C√†i ƒë·∫∑t containerd container runtime
  
	  - name: Create containerd configuration directory
		file:
		  path: /etc/containerd
		  state: directory
		# T·∫°o th∆∞ m·ª•c c·∫•u h√¨nh containerd
  
	  - name: Generate default containerd configuration
		shell: "containerd config default > /etc/containerd/config.toml"
		# T·∫°o file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd
  
	  - name: Enable SystemdCgroup
		replace:
		  path: /etc/containerd/config.toml
		  regexp: 'SystemdCgroup = false'
		  replace: 'SystemdCgroup = true'
		# B·∫≠t SystemdCgroup trong containerd
  
	  - name: Restart containerd service
		systemd:
		  name: containerd
		  enabled: yes
		  state: restarted
		# Kh·ªüi ƒë·ªông l·∫°i containerd`,
  
	  '04-install-kubernetes': `---
  - hosts: all
	become: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Install required packages
		apt:
		  name:
			- apt-transport-https
			- ca-certificates
			- curl
		  state: present
		  update_cache: yes
		# C√†i c√°c g√≥i ph·ª• thu·ªôc c·∫ßn thi·∫øt
  
	  - name: Add Kubernetes GPG key
		shell: |
		  if [ ! -f /usr/share/keyrings/kubernetes-archive-keyring.gpg ]; then
			curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \\
			gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
		  else
			echo "GPG key ƒë√£ t·ªìn t·∫°i, b·ªè qua b∆∞·ªõc n√†y."
		  fi
		changed_when: false
		register: gpg_status  
		# Th√™m GPG key ch√≠nh th·ª©c c·ªßa Kubernetes
  
	  - name: Add Kubernetes repository
		copy:
		  dest: /etc/apt/sources.list.d/kubernetes.list
		  content: |
			deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
		# Th√™m repository APT c·ªßa Kubernetes
  
	  - name: Install kubelet, kubeadm, kubectl
		apt:
		  name:
			- kubelet
			- kubeadm
			- kubectl
		  state: present
		  update_cache: yes
		# C√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn core c·ªßa Kubernetes
  
	  - name: Hold package versions
		command: apt-mark hold kubelet kubeadm kubectl
		# Gi·ªØ phi√™n b·∫£n c√°c package ƒë·ªÉ tr√°nh c·∫≠p nh·∫≠t t·ª± ƒë·ªông`,
  
	  '05-init-master': `---
  - hosts: master
	become: yes
	gather_facts: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
  
	vars:
	  pod_network_cidr: "10.244.0.0/16"
	  calico_manifest: "https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/calico.yaml"
	  join_script: "/etc/kubernetes/join-command.sh"
  
	tasks:
	  - name: Get dynamic master IP address
		set_fact:
		  master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
		# L·∫•y ƒë·ªãa ch·ªâ IP ƒë·ªông c·ªßa master
  
	  - name: Display master IP being used
		debug:
		  msg: "S·ª≠ d·ª•ng ƒë·ªãa ch·ªâ master: {{ master_ip }}"
		# Hi·ªÉn th·ªã ƒë·ªãa ch·ªâ master ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng
  
	  - name: Reset old cluster and clean up data
		shell: |
		  kubeadm reset -f || true
		  rm -rf /etc/kubernetes /var/lib/etcd /var/lib/kubelet /etc/cni/net.d
		  systemctl restart containerd || true
		ignore_errors: yes
		# Reset c·ª•m c≈© v√† d·ªçn s·∫°ch d·ªØ li·ªáu
  
	  - name: Initialize Kubernetes Control Plane
		command: >
		  kubeadm init
		  --control-plane-endpoint "{{ master_ip }}:6443"
		  --apiserver-advertise-address {{ master_ip }}
		  --pod-network-cidr {{ pod_network_cidr }}
		  --upload-certs
		args:
		  creates: /etc/kubernetes/admin.conf
		register: kubeadm_init
		failed_when: "'error' in kubeadm_init.stderr"
		changed_when: "'Your Kubernetes control-plane has initialized successfully' in kubeadm_init.stdout"
		# Kh·ªüi t·∫°o Control Plane v·ªõi Calico CNI
  
	  - name: Configure kubeconfig for root user
		shell: |
		  mkdir -p $HOME/.kube
		  cp /etc/kubernetes/admin.conf $HOME/.kube/config
		  chown $(id -u):$(id -g) $HOME/.kube/config
		args:
		  executable: /bin/bash
		# C·∫•u h√¨nh kubeconfig cho root
  
	  - name: Configure kubeconfig for normal user
		when: ansible_user != "root"
		block:
		  - name: Create kubeconfig directory for user
			file:
			  path: "/home/{{ ansible_user }}/.kube"
			  state: directory
			  mode: '0755'
			# T·∫°o th∆∞ m·ª•c kubeconfig cho user
  
		  - name: Copy kubeconfig for user
			copy:
			  src: /etc/kubernetes/admin.conf
			  dest: "/home/{{ ansible_user }}/.kube/config"
			  owner: "{{ ansible_user }}"
			  group: "{{ ansible_user }}"
			  mode: '0600'
			  remote_src: yes
			# Sao ch√©p kubeconfig cho user
		# C·∫•u h√¨nh kubeconfig cho user th∆∞·ªùng (n·∫øu kh√¥ng ph·∫£i root)
  
	  - name: Generate join command for workers
		shell: kubeadm token create --print-join-command
		register: join_cmd
		changed_when: false
		# Sinh l·ªánh join cho worker
  
	  - name: Save join command to file
		copy:
		  content: "{{ join_cmd.stdout }}"
		  dest: "{{ join_script }}"
		  mode: '0755'
		# L∆∞u l·ªánh join ra file
  
	  - name: Display join command
		debug:
		  msg:
			- "L·ªánh join worker:"
			- "{{ join_cmd.stdout }}"
			- "File l∆∞u t·∫°i: {{ join_script }}"
		# Hi·ªÉn th·ªã join command
  
	  - name: Complete master initialization
		debug:
		  msg: "Master {{ inventory_hostname }} ƒë√£ s·∫µn s√†ng cho worker join!"
		# Ho√†n t·∫•t kh·ªüi t·∫°o master`,
  
	  '06-install-cni': `---
  - name: C√†i ƒë·∫∑t ho·∫∑c c·∫≠p nh·∫≠t Calico CNI (t·ª± ƒë·ªông)
	hosts: master
	become: yes
	gather_facts: false
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
  
	vars:
	  calico_version: "v3.27.3"
	  calico_url: "https://raw.githubusercontent.com/projectcalico/calico/{{ calico_version }}/manifests/calico.yaml"
  
	tasks:
	  - name: Ki·ªÉm tra Calico CNI c√≥ t·ªìn t·∫°i kh√¥ng
		command: kubectl get daemonset calico-node -n kube-system
		register: calico_check
		ignore_errors: true
  
	  - name: Hi·ªÉn th·ªã tr·∫°ng th√°i hi·ªán t·∫°i
		debug:
		  msg: >
			{% if calico_check.rc == 0 %}
			  Calico ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.
			{% else %}
			  Ch∆∞a c√≥ Calico, s·∫Ω ti·∫øn h√†nh c√†i ƒë·∫∑t m·ªõi.
			{% endif %}
  
	  - name: Ki·ªÉm tra kernel modules overlay & br_netfilter
		shell: |
		  modprobe overlay || true
		  modprobe br_netfilter || true
		  lsmod | grep -E 'overlay|br_netfilter' || echo "Thi·∫øu module kernel"
		register: kernel_status
		ignore_errors: true
  
	  - name: K·∫øt qu·∫£ ki·ªÉm tra module kernel
		debug:
		  var: kernel_status.stdout_lines
  
	  - name: Ki·ªÉm tra c·∫•u h√¨nh sysctl
		shell: |
		  echo "net.bridge.bridge-nf-call-iptables = 1" | tee /etc/sysctl.d/k8s.conf >/dev/null
		  echo "net.ipv4.ip_forward = 1" | tee -a /etc/sysctl.d/k8s.conf >/dev/null
		  sysctl --system | grep -E "net.bridge.bridge-nf-call|net.ipv4.ip_forward"
		register: sysctl_status
		ignore_errors: true
  
	  - name: K·∫øt qu·∫£ sysctl
		debug:
		  var: sysctl_status.stdout_lines
  
	  - name: √Åp d·ª•ng Calico manifest (c√†i m·ªõi ho·∫∑c c·∫≠p nh·∫≠t)
		shell: |
		  kubectl apply -f {{ calico_url }}
		args:
		  executable: /bin/bash
		register: calico_apply
		retries: 3
		delay: 10
		until: calico_apply.rc == 0
  
	  - name: Hi·ªÉn th·ªã k·∫øt qu·∫£ c√†i ƒë·∫∑t
		debug:
		  var: calico_apply.stdout_lines
  
	  - name: Ki·ªÉm tra Calico node pod ƒëang kh·ªüi ƒë·ªông
		shell: |
		  kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c 'Running' || true
		register: calico_running
  
	  - name: Ch·ªù pod kh·ªüi ƒë·ªông (t·ªëi ƒëa 10 l·∫ßn)
		until: calico_running.stdout | int > 0
		retries: 10
		delay: 15
		shell: |
		  kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c 'Running' || true
		register: calico_running
		ignore_errors: true
  
	  - name: X√°c nh·∫≠n Calico pods ƒëang ch·∫°y
		when: calico_running.stdout | int > 0
		debug:
		  msg: "Calico ƒëang ho·∫°t ƒë·ªông ({{ calico_running.stdout }} pods Running)."
  
	  - name: Log pod Calico n·∫øu l·ªói
		when: calico_running.stdout | int == 0
		shell: kubectl logs -n kube-system -l k8s-app=calico-node --tail=50 || true
		register: calico_logs
		ignore_errors: true
  
	  - name: Hi·ªÉn th·ªã log pod Calico
		when: calico_running.stdout | int == 0
		debug:
		  msg: "{{ calico_logs.stdout_lines | default(['Pod Calico ch∆∞a s·∫µn s√†ng ho·∫∑c kh√¥ng c√≥ log.']) }}"
  
	  - name: Ki·ªÉm tra tr·∫°ng th√°i node
		command: kubectl get nodes -o wide
		register: nodes_status
		ignore_errors: true
  
	  - name: Hi·ªÉn th·ªã k·∫øt qu·∫£ cluster
		debug:
		  var: nodes_status.stdout_lines`,
  
	  '06-install-flannel': `---
  - name: C√†i ƒë·∫∑t ho·∫∑c c·∫≠p nh·∫≠t Flannel CNI (t∆∞∆°ng th√≠ch WSL2)
	hosts: master
	become: yes
	gather_facts: false
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
  
	vars:
	  flannel_manifest: "https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml"
  
	tasks:
	  - name: Ki·ªÉm tra Flannel CNI c√≥ t·ªìn t·∫°i kh√¥ng
		command: kubectl get daemonset kube-flannel-ds -n kube-flannel
		register: flannel_check
		ignore_errors: true
  
	  - name: Hi·ªÉn th·ªã tr·∫°ng th√°i hi·ªán t·∫°i
		debug:
		  msg: >
			{% if flannel_check.rc == 0 %}
			  Flannel ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t tr∆∞·ªõc ƒë√≥.
			{% else %}
			  Ch∆∞a c√≥ Flannel, s·∫Ω ti·∫øn h√†nh c√†i ƒë·∫∑t m·ªõi.
			{% endif %}
  
	  - name: B·∫≠t IP forwarding
		shell: |
		  echo "net.ipv4.ip_forward = 1" | tee /etc/sysctl.d/k8s.conf >/dev/null
		  sysctl --system | grep net.ipv4.ip_forward
		register: sysctl_status
		ignore_errors: true
  
	  - name: K·∫øt qu·∫£ sysctl
		debug:
		  var: sysctl_status.stdout_lines
  
	  - name: √Åp d·ª•ng Flannel manifest (t·ª± ƒë·ªông t·∫£i b·∫£n m·ªõi nh·∫•t)
		command: kubectl apply -f {{ flannel_manifest }}
		register: flannel_apply
		changed_when: "'created' in flannel_apply.stdout or 'configured' in flannel_apply.stdout"
		failed_when: flannel_apply.rc != 0
  
	  - name: Hi·ªÉn th·ªã k·∫øt qu·∫£ √°p d·ª•ng
		debug:
		  var: flannel_apply.stdout_lines
  
	  - name: Ki·ªÉm tra s·ªë pod Flannel ƒëang ch·∫°y
		shell: |
		  kubectl get pods -n kube-flannel --no-headers 2>/dev/null | grep -c 'Running' || true
		register: flannel_running
  
	  - name: Ch·ªù pod Flannel ho·∫°t ƒë·ªông (t·ªëi ƒëa 10 l·∫ßn)
		until: flannel_running.stdout | int > 0
		retries: 10
		delay: 15
		shell: |
		  kubectl get pods -n kube-flannel --no-headers 2>/dev/null | grep -c 'Running' || true
		register: flannel_running
		ignore_errors: true
  
	  - name: X√°c nh·∫≠n Flannel pod ƒë√£ ho·∫°t ƒë·ªông
		when: flannel_running.stdout | int > 0
		debug:
		  msg: "Flannel ƒëang ho·∫°t ƒë·ªông ({{ flannel_running.stdout }} pods Running)."
  
	  - name: Log Flannel n·∫øu pod ch∆∞a ch·∫°y
		when: flannel_running.stdout | int == 0
		shell: kubectl logs -n kube-flannel -l app=flannel --tail=50 || true
		register: flannel_logs
		ignore_errors: true
  
	  - name: Hi·ªÉn th·ªã log Flannel
		when: flannel_running.stdout | int == 0
		debug:
		  msg: "{{ flannel_logs.stdout_lines | default(['Pod Flannel ch∆∞a s·∫µn s√†ng ho·∫∑c kh√¥ng c√≥ log.']) }}"
  
	  - name: Ki·ªÉm tra tr·∫°ng th√°i node
		command: kubectl get nodes -o wide
		register: nodes_status
		ignore_errors: true
  
	  - name: Hi·ªÉn th·ªã k·∫øt qu·∫£ cluster
		debug:
		  var: nodes_status.stdout_lines`,
  
	  '07-join-workers': `---
  - hosts: workers
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	vars:
	  join_script: /tmp/kube_join.sh
	tasks:
	  - name: Test SSH connectivity to worker node
		ping:
		register: ping_result
		ignore_errors: yes
		# Ki·ªÉm tra k·∫øt n·ªëi SSH ƒë·∫øn worker node
  
	  - name: Skip offline workers
		set_fact:
		  worker_online: "{{ ping_result is succeeded }}"
		# ƒê√°nh d·∫•u worker n√†o online/offline
  
	  - name: Display worker online status
		debug:
		  msg: "Worker {{ inventory_hostname }} is {{ 'ONLINE' if worker_online else 'OFFLINE' }}"
		# Hi·ªÉn th·ªã tr·∫°ng th√°i online/offline
  
	  - name: Get join command from master
		delegate_to: "{{ groups['master'][0] }}"
		run_once: true
		shell: kubeadm token create --print-join-command
		register: join_cmd
		when: worker_online
		# L·∫•y l·ªánh join t·ª´ master node (ch·ªâ ch·∫°y 1 l·∫ßn)
  
	  - name: Save join command to file
		copy:
		  content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
		  dest: "{{ join_script }}"
		  mode: '0755'
		when: worker_online
		ignore_errors: yes
		# Ghi l·ªánh join ra file script
  
	  - name: Reset node if old cluster exists
		shell: |
		  kubeadm reset -f || true
		  rm -rf /etc/kubernetes /var/lib/kubelet /etc/cni/net.d
		  systemctl restart containerd || true
		ignore_errors: yes
		when: worker_online
		# Reset node c≈© (n·∫øu c√≥)
  
	  - name: Join to Kubernetes cluster
		shell: "{{ join_script }}"
		register: join_output
		ignore_errors: yes
		when: worker_online
		# Th·ª±c thi l·ªánh join v√†o cluster
  
	  - name: Display join result
		debug:
		  msg: "{{ join_output.stdout_lines | default(['ƒê√£ join th√†nh c√¥ng!']) if worker_online else ['Worker offline, skip join'] }}"
		# Hi·ªÉn th·ªã k·∫øt qu·∫£ join
  
	  - name: Restart kubelet service
		systemd:
		  name: kubelet
		  state: restarted
		  enabled: yes
		ignore_errors: yes
		when: worker_online
		# Kh·ªüi ƒë·ªông l·∫°i kubelet
  
	  - name: Complete join process
		debug:
		  msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"
		# B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng`,
  
	  '09-install-ingress': `---
  - hosts: master
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
  
	tasks:
	  - name: L·∫•y ƒë·ªãa ch·ªâ master ƒë·ªông
		set_fact:
		  master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
	  - debug:
		  msg: "C√†i ƒë·∫∑t Ingress tr√™n master: {{ master_ip }}"
  
	  - name: C√†i ƒë·∫∑t Ingress Controller (nginx)
		shell: |
		  KUBECONFIG=/etc/kubernetes/admin.conf \
		  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
		args:
		  executable: /bin/bash
		register: ingress_install
		ignore_errors: yes
  
	  - name: K·∫øt qu·∫£ c√†i Ingress
		debug:
		  msg: "{{ ingress_install.stdout_lines | default(['Ingress Controller applied']) }}"
  
	  - name: Ki·ªÉm tra tr·∫°ng th√°i pod ingress-nginx
		shell: |
		  KUBECONFIG=/etc/kubernetes/admin.conf \
		  kubectl get pods -n ingress-nginx -o wide
		register: ingress_pods
  
	  - name: Hi·ªÉn th·ªã pod ingress-nginx
		debug:
		  msg: "{{ ingress_pods.stdout_lines }}"
  
	  - name: Ho√†n t·∫•t
		debug:
		  msg: "Ingress Controller (NGINX) ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t th√†nh c√¥ng!"`,
  
	  '11-install-helm': `---
  - hosts: master
	become: yes
	gather_facts: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
  
	tasks:
	  - name: C√†i ƒë·∫∑t Helm n·∫øu ch∆∞a c√≥
		shell: |
		  curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
		args:
		  executable: /bin/bash
		register: helm_install
		ignore_errors: yes
  
	  - name: K·∫øt qu·∫£ c√†i ƒë·∫∑t Helm
		debug:
		  msg: "{{ helm_install.stdout_lines | default(['Helm installed']) }}"
  
	  - name: Ki·ªÉm tra phi√™n b·∫£n Helm
		shell: helm version --short
		register: helm_version
  
	  - name: Hi·ªÉn th·ªã th√¥ng tin Helm
		debug:
		  msg: "Phi√™n b·∫£n Helm hi·ªán t·∫°i: {{ helm_version.stdout | default('Kh√¥ng x√°c ƒë·ªãnh') }}"
  
	  - name: Ho√†n t·∫•t
		debug:
		  msg: "Helm ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t th√†nh c√¥ng tr√™n master!"`,
  
	  '10-install-metallb': `---
  - name: Install and configure MetalLB on Kubernetes
	hosts: master
	become: yes
	gather_facts: yes
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
	vars:
	  metallb_version: "v0.14.8"
	  metallb_namespace: "metallb-system"
	  metallb_url: "https://raw.githubusercontent.com/metallb/metallb/{{ metallb_version }}/config/manifests/metallb-native.yaml"
	tasks:
	  - name: Get master IP address dynamically
		set_fact:
		  master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
		# L·∫•y ƒë·ªãa ch·ªâ IP ƒë·ªông c·ªßa master node
  
	  - name: Calculate IP range from master IP subnet using shell
		shell: |
		  MASTER_IP="{{ master_ip }}"
		  SUBNET=$(echo "$MASTER_IP" | cut -d'.' -f1-3)
		  echo "\${SUBNET}.240"
		  echo "\${SUBNET}.250"
		register: ip_range_result
		changed_when: false
		# T·ª± ƒë·ªông t√≠nh to√°n IP range t·ª´ subnet c·ªßa master IP (240-250)
  
	  - name: Extract IP range start and end
		set_fact:
		  ip_range_start: "{{ ip_range_result.stdout_lines[0] }}"
		  ip_range_end: "{{ ip_range_result.stdout_lines[1] }}"
		# Tr√≠ch xu·∫•t IP range start v√† end t·ª´ k·∫øt qu·∫£ shell
  
	  - name: Display calculated MetalLB IP range
		debug:
		  msg:
			- "Master IP: {{ master_ip }}"
			- "Auto-detected MetalLB IP Pool: {{ ip_range_start }} - {{ ip_range_end }}"
			- "IP range ƒë∆∞·ª£c t√≠nh t·ª± ƒë·ªông t·ª´ network c·ªßa master node"
		# Hi·ªÉn th·ªã IP range ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n
  
	  - name: Check if MetalLB namespace exists
		command: kubectl get namespace {{ metallb_namespace }}
		register: ns_check
		failed_when: false
		changed_when: false
		# Ki·ªÉm tra namespace MetalLB ƒë√£ t·ªìn t·∫°i
  
	  - name: Create MetalLB namespace if missing
		command: kubectl create namespace {{ metallb_namespace }}
		when: ns_check.rc != 0
		changed_when: true
		# T·∫°o namespace cho MetalLB (n·∫øu ch∆∞a c√≥)
  
	  - name: Apply MetalLB official manifest
		command: kubectl apply -f {{ metallb_url }}
		register: metallb_apply
		changed_when: "'created' in metallb_apply.stdout or 'configured' in metallb_apply.stdout"
		# √Åp d·ª•ng manifest ch√≠nh th·ª©c c·ªßa MetalLB
  
	  - name: Wait for MetalLB controller pods to start
		shell: |
		  kubectl get pods -n {{ metallb_namespace }} -l component=controller --no-headers | grep -c 'Running' || true
		register: metallb_running
		until: metallb_running.stdout | int > 0
		retries: 10
		delay: 10
		ignore_errors: true
		# Ch·ªù controller pods c·ªßa MetalLB kh·ªüi ƒë·ªông
  
	  - name: Create MetalLB IPAddressPool manifest with auto-detected IP range
		copy:
		  dest: /tmp/metallb-ip-pool.yaml
		  content: |
			apiVersion: metallb.io/v1beta1
			kind: IPAddressPool
			metadata:
			  name: default-address-pool
			  namespace: {{ metallb_namespace }}
			spec:
			  addresses:
				- {{ ip_range_start }}-{{ ip_range_end }}
		# T·∫°o manifest IPAddressPool v·ªõi IP range t·ª± ƒë·ªông t·ª´ master node
  
	  - name: Apply IPAddressPool manifest
		command: kubectl apply -f /tmp/metallb-ip-pool.yaml
		register: ip_pool_apply
		changed_when: "'created' in ip_pool_apply.stdout or 'configured' in ip_pool_apply.stdout"
		# √Åp d·ª•ng c·∫•u h√¨nh IPAddressPool
  
	  - name: Create L2Advertisement manifest
		copy:
		  dest: /tmp/metallb-l2advertisement.yaml
		  content: |
			apiVersion: metallb.io/v1beta1
			kind: L2Advertisement
			metadata:
			  name: default-advertisement
			  namespace: {{ metallb_namespace }}
			spec:
			  ipAddressPools:
				- default-address-pool
		# T·∫°o manifest L2Advertisement cho MetalLB
  
	  - name: Apply L2Advertisement manifest
		command: kubectl apply -f /tmp/metallb-l2advertisement.yaml
		register: l2_apply
		changed_when: "'created' in l2_apply.stdout or 'configured' in l2_apply.stdout"
		# √Åp d·ª•ng c·∫•u h√¨nh L2Advertisement
  
	  - name: Show MetalLB pods and IP configuration
		shell: |
		  echo "=== MetalLB Pods ==="
		  kubectl get pods -n {{ metallb_namespace }}
		  echo ""
		  echo "=== IPAddressPools ==="
		  kubectl get ipaddresspools -n {{ metallb_namespace }}
		register: metallb_status
		changed_when: false
		# Hi·ªÉn th·ªã pods v√† c·∫•u h√¨nh IP c·ªßa MetalLB
  
	  - name: Display summary
		debug:
		  msg:
			- "MetalLB installed successfully."
			- "Namespace: {{ metallb_namespace }}"
			- "Master IP: {{ master_ip }}"
			- "Auto-detected IP Pool: {{ ip_range_start }} - {{ ip_range_end }}"
			- "{{ metallb_status.stdout_lines }}"
		# Ho√†n t·∫•t c√†i ƒë·∫∑t MetalLB LoadBalancer v·ªõi IP range t·ª± ƒë·ªông`,
  
	  '12-setup-storage': `---
  - hosts: master
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
  
	vars:
	  nfs_manifest_dir: /etc/kubernetes/storage
  
	tasks:
	  - name: T·∫°o th∆∞ m·ª•c manifest NFS
		file:
		  path: "{{ nfs_manifest_dir }}"
		  state: directory
		  mode: '0755'
  
	  - name: T·∫£i v√† √°p d·ª•ng NFS Provisioner (example)
		shell: |
		  KUBECONFIG=/etc/kubernetes/admin.conf \
		  kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/rbac.yaml
		  KUBECONFIG=/etc/kubernetes/admin.conf \
		  kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/deployment.yaml
		args:
		  executable: /bin/bash
		register: nfs_apply
		ignore_errors: yes
  
	  - name: K·∫øt qu·∫£ tri·ªÉn khai NFS
		debug:
		  msg: "{{ nfs_apply.stdout_lines | default(['NFS Provisioner applied']) }}"
  
	  - name: ƒê·∫∑t StorageClass m·∫∑c ƒë·ªãnh
		shell: |
		  KUBECONFIG=/etc/kubernetes/admin.conf \
		  kubectl patch storageclass nfs-client -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' || true
		args:
		  executable: /bin/bash
  
	  - name: Ho√†n t·∫•t
		debug:
		  msg: "C·∫•u h√¨nh StorageClass (NFS) m·∫∑c ƒë·ªãnh ƒë√£ ho√†n t·∫•t!"`,
  
	  '13-prepare-and-join-worker': `---
  # All-in-one: Chu·∫©n b·ªã node v√† join v√†o c·ª•m (02 ‚Üí 03 ‚Üí 04 ‚Üí 07)
  
  # Precheck: Ch·ªâ ƒë·ªãnh nh√≥m target_workers g·ªìm c√°c worker ch∆∞a Ready ho·∫∑c ch∆∞a c√≥ trong kubectl
  - name: Precheck not-ready workers
	hosts: master
	gather_facts: no
	tasks:
	  - name: Check if kubectl is available
		command: which kubectl
		register: kubectl_check
		failed_when: false
		changed_when: false
  
	  - name: Get existing node list
		command: kubectl get nodes --no-headers
		register: nodes_tbl
		changed_when: false
		failed_when: false
		when: kubectl_check.rc == 0
  
	  - name: Ensure node_lines fact always exists
		set_fact:
		  node_lines: "{{ nodes_tbl.stdout_lines | default([]) if (nodes_tbl is defined and nodes_tbl.stdout_lines is defined) else [] }}"
  
	  - name: Parse node information safely
		set_fact:
		  node_names: "{{ node_lines | map('split') | map('first') | list | default([]) }}"
		  not_ready_names: "{{ node_lines | map('split') | selectattr('1','ne','Ready') | map('first') | list | default([]) }}"
  
	  - name: Add unregistered or not-ready workers to target group
		add_host:
		  name: "{{ item }}"
		  groups: target_workers
		loop: "{{ groups['workers'] | default([]) }}"
		when: kubectl_check.rc != 0 or
			  nodes_tbl is not defined or
			  item not in (node_names | default([])) or
			  item in (not_ready_names | default([]))
  
  - name: 02 - C·∫•u h√¨nh kernel v√† sysctl
	hosts: target_workers
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: T·∫Øt swap
		shell: swapoff -a || true
		ignore_errors: true
  
	  - name: Comment d√≤ng swap trong /etc/fstab
		replace:
		  path: /etc/fstab
		  regexp: '(^.*swap.*$)'
		  replace: '# \\1'
		ignore_errors: yes
  
	  - name: T·∫°o file modules-load cho containerd
		copy:
		  dest: /etc/modules-load.d/containerd.conf
		  content: |
			overlay
			br_netfilter
  
	  - name: K√≠ch ho·∫°t module overlay v√† br_netfilter
		shell: |
		  modprobe overlay || true
		  modprobe br_netfilter || true
  
	  - name: C·∫•u h√¨nh sysctl cho Kubernetes
		copy:
		  dest: /etc/sysctl.d/99-kubernetes-cri.conf
		  content: |
			net.bridge.bridge-nf-call-iptables  = 1
			net.bridge.bridge-nf-call-ip6tables = 1
			net.ipv4.ip_forward                 = 1
  
	  - name: √Åp d·ª•ng sysctl
		command: sysctl --system
		ignore_errors: yes
  
  - name: 03 - C√†i ƒë·∫∑t v√† c·∫•u h√¨nh containerd
	hosts: target_workers
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: C·∫≠p nh·∫≠t cache APT
		apt:
		  update_cache: yes
  
	  - name: C√†i containerd
		apt:
		  name: containerd
		  state: present
		  force_apt_get: yes
  
	  - name: T·∫°o th∆∞ m·ª•c c·∫•u h√¨nh containerd
		file:
		  path: /etc/containerd
		  state: directory
  
	  - name: Sinh file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd
		shell: "containerd config default > /etc/containerd/config.toml"
  
	  - name: B·∫≠t SystemdCgroup
		replace:
		  path: /etc/containerd/config.toml
		  regexp: 'SystemdCgroup = false'
		  replace: 'SystemdCgroup = true'
  
	  - name: Kh·ªüi ƒë·ªông l·∫°i containerd
		systemd:
		  name: containerd
		  enabled: yes
		  state: restarted
  
  - name: 04 - C√†i ƒë·∫∑t Kubernetes (kubelet, kubeadm, kubectl)
	hosts: target_workers
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Th√™m GPG key c·ªßa Kubernetes (n·∫øu ch∆∞a c√≥)
		shell: |
		  if [ ! -f /usr/share/keyrings/kubernetes-archive-keyring.gpg ]; then
			curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
			gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
		  fi
		changed_when: false
		ignore_errors: true
  
	  - name: Th√™m repository Kubernetes
		copy:
		  dest: /etc/apt/sources.list.d/kubernetes.list
		  content: |
			deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
  
	  - name: C√†i kubelet, kubeadm, kubectl
		apt:
		  name:
			- kubelet
			- kubeadm
			- kubectl
		  state: present
		  update_cache: yes
  
	  - name: Gi·ªØ phi√™n b·∫£n kubelet/kubeadm/kubectl
		command: apt-mark hold kubelet kubeadm kubectl
  
  - name: 07 - Join worker v√†o c·ª•m
	hosts: target_workers
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	vars:
	  join_script: /tmp/kube_join.sh
	tasks:
	  - name: Ki·ªÉm tra k·∫øt n·ªëi SSH t·ªõi worker
		ping:
		register: ping_result
		ignore_errors: yes
  
	  - name: ƒê√°nh d·∫•u tr·∫°ng th√°i online
		set_fact:
		  worker_online: "{{ ping_result is succeeded }}"
  
	  - name: L·∫•y join command t·ª´ master
		delegate_to: "{{ groups['master'][0] }}"
		run_once: true
		shell: kubeadm token create --print-join-command
		register: join_cmd
		when: worker_online
  
	  - name: Ghi join command ra file
		copy:
		  content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
		  dest: "{{ join_script }}"
		  mode: '0755'
		when: worker_online
		ignore_errors: yes
  
	  - name: Reset node c≈© (n·∫øu c√≥)
		shell: |
		  kubeadm reset -f || true
		  rm -rf /etc/kubernetes /var/lib/kubelet /etc/cni/net.d
		  systemctl restart containerd || true
		ignore_errors: yes
		when: worker_online
  
	  - name: Th·ª±c thi l·ªánh join
		shell: "{{ join_script }}"
		register: join_output
		ignore_errors: yes
		when: worker_online
  
	  - name: Kh·ªüi ƒë·ªông l·∫°i kubelet
		systemd:
		  name: kubelet
		  state: restarted
		  enabled: yes
		ignore_errors: yes
		when: worker_online
  
	  - name: T·ªïng k·∫øt k·∫øt qu·∫£
		debug:
		  msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"`,
  
  
  
	  '08-verify-cluster': `---
  - name: Ki·ªÉm tra tr·∫°ng th√°i c·ª•m Kubernetes
	hosts: master
	become: yes
	gather_facts: no
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
  
	tasks:
	  - name: Ki·ªÉm tra kubectl c√≥ s·∫µn kh√¥ng
		command: which kubectl
		register: kubectl_check
		failed_when: kubectl_check.rc != 0
		changed_when: false
  
	  - name: Li·ªát k√™ danh s√°ch node
		command: kubectl get nodes
		register: nodes_info
		changed_when: false
  
	  - name: Li·ªát k√™ pods h·ªá th·ªëng
		command: kubectl get pods -n kube-system
		register: pods_info
		changed_when: false
  
	  - name: Hi·ªÉn th·ªã th√¥ng tin c·ª•m
		debug:
		  msg:
			- "Danh s√°ch Node:"
			- "{{ nodes_info.stdout_lines }}"
			- "Pods trong namespace kube-system:"
			- "{{ pods_info.stdout_lines }}"
  
	  - name: Ki·ªÉm tra tr·∫°ng th√°i node
		shell: kubectl get nodes --no-headers | awk '{print $2}' | sort | uniq -c
		register: node_status
		changed_when: false
  
	  - name: B√°o c√°o t√¨nh tr·∫°ng node
		debug:
		  msg: |
			{% if 'NotReady' in node_status.stdout %}
			M·ªôt s·ªë node ch∆∞a s·∫µn s√†ng:
			{{ node_status.stdout }}
			{% else %}
			T·∫•t c·∫£ node ƒë√£ ·ªü tr·∫°ng th√°i Ready!
			{% endif %}
  
	  - name: Ki·ªÉm tra pod l·ªói trong kube-system
		shell: kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' || true
		register: bad_pods
		changed_when: false
  
	  - name: B√°o c√°o pod l·ªói
		debug:
		  msg: |
			{% if bad_pods.stdout %}
			M·ªôt s·ªë pod ch∆∞a ·ªïn ƒë·ªãnh ho·∫∑c ƒëang l·ªói:
			{{ bad_pods.stdout }}
			{% else %}
			T·∫•t c·∫£ pod trong kube-system ƒë·ªÅu ƒëang Running ho·∫∑c Completed!
			{% endif %}
  
	  - name: Hi·ªÉn th·ªã log c·ªßa pod l·ªói (n·∫øu c√≥)
		when: bad_pods.stdout != ""
		shell: |
		  for pod in $(kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' | awk '{print $1}'); do
			echo "Log c·ªßa $pod:"; kubectl logs -n kube-system $pod --tail=30 || true; echo "--------------------------------";
		  done
		register: bad_pods_logs
		ignore_errors: yes
  
	  - name: Log chi ti·∫øt
		when: bad_pods.stdout != ""
		debug:
		  msg: "{{ bad_pods_logs.stdout_lines | default(['Kh√¥ng c√≥ log l·ªói']) }}"`,
  
	  '00-reset-cluster': `---
  - name: Reset entire Kubernetes cluster
	hosts: all
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Reset Kubernetes cluster
		shell: kubeadm reset -f
		ignore_errors: true
		register: reset_output
		# G·ª° c·ª•m Kubernetes (kubeadm reset -f)
  
	  - name: Display reset results
		debug:
		  msg: "{{ reset_output.stdout_lines | default(['Kh√¥ng c√≥ cluster c≈© ƒë·ªÉ reset.']) }}"
		# Hi·ªÉn th·ªã k·∫øt qu·∫£ reset
  
	  - name: Remove Kubernetes configuration directory
		file:
		  path: /etc/kubernetes
		  state: absent
		# X√≥a th∆∞ m·ª•c c·∫•u h√¨nh Kubernetes
  
	  - name: Remove CNI network configuration
		file:
		  path: /etc/cni/net.d
		  state: absent
		# X√≥a c·∫•u h√¨nh m·∫°ng CNI
  
	  - name: Remove root kubeconfig
		file:
		  path: /root/.kube
		  state: absent
		# X√≥a file kubeconfig c·ªßa root
  
	  - name: Remove normal user kubeconfig
		file:
		  path: "/home/{{ ansible_user }}/.kube"
		  state: absent
		when: ansible_user != "root"
		# X√≥a file kubeconfig c·ªßa user th∆∞·ªùng
  
	  - name: Clean up iptables rules
		shell: |
		  iptables -F && iptables -X
		  iptables -t nat -F && iptables -t nat -X
		  iptables -t mangle -F && iptables -t mangle -X
		  iptables -P FORWARD ACCEPT
		ignore_errors: true
		# D·ªçn iptables
  
	  - name: Restart containerd service
		systemd:
		  name: containerd
		  state: restarted
		  enabled: yes
		# Kh·ªüi ƒë·ªông l·∫°i containerd
  
	  - name: Confirm reset completed
		debug:
		  msg:
			- "Node {{ inventory_hostname }} ƒë√£ ƒë∆∞·ª£c reset s·∫°ch (ch·ªâ x√≥a d·ªØ li·ªáu)."
		# X√°c nh·∫≠n reset ho√†n t·∫•t`,
  
	  'deploy-full-cluster': `---
  - name: Step 0 - Reset Kubernetes cluster
	hosts: all
	become: yes
	gather_facts: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Reset old cluster if exists
		shell: kubeadm reset -f || true
		ignore_errors: true
		# Reset c·ª•m Kubernetes c≈© n·∫øu t·ªìn t·∫°i
  
	  - name: Remove old Kubernetes configuration
		file:
		  path: "{{ item }}"
		  state: absent
		loop:
		  - /etc/kubernetes
		  - /etc/cni/net.d
		  - /root/.kube
		  - "/home/{{ ansible_user }}/.kube"
		ignore_errors: true
		# X√≥a c√°c th∆∞ m·ª•c c·∫•u h√¨nh Kubernetes c≈©
  
	  - name: Restart containerd service
		shell: systemctl restart containerd || true
		ignore_errors: true
		# Kh·ªüi ƒë·ªông l·∫°i containerd ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch s·∫Ω
  
  - name: Step 1 - Update /etc/hosts and hostname
	hosts: all
	become: yes
	gather_facts: yes
	tasks:
	  - name: Update /etc/hosts file for all nodes
		lineinfile:
		  path: /etc/hosts
		  line: "{{ hostvars[item].ansible_host }} {{ item }}"
		  state: present
		  create: yes
		  insertafter: EOF
		loop: "{{ groups['all'] }}"
		when: hostvars[item].ansible_host is defined
		# Th√™m danh s√°ch c√°c node v√†o /etc/hosts ƒë·ªÉ cluster nh·∫≠n di·ªán nhau
  
	  - name: Set hostname according to inventory
		hostname:
		  name: "{{ inventory_hostname }}"
		when: ansible_hostname != inventory_hostname
		# ƒê·∫∑t hostname theo t√™n trong inventory
  
	  - name: Verify hostname
		shell: hostnamectl
		register: host_info
		# Ki·ªÉm tra hostname ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t ƒë√∫ng
  
	  - name: Display hostname info
		debug:
		  msg: "{{ host_info.stdout_lines }}"
		# Hi·ªÉn th·ªã th√¥ng tin hostname
  
  - name: Step 2 - Configure kernel and containerd
	hosts: all
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Disable swap
		shell: swapoff -a && sed -i '/swap/d' /etc/fstab || true
		# T·∫Øt swap v√¨ Kubernetes kh√¥ng h·ªó tr·ª£
  
	  - name: Load kernel modules for containerd
		copy:
		  dest: /etc/modules-load.d/containerd.conf
		  content: |
			overlay
			br_netfilter
		# T·∫£i c√°c kernel module c·∫ßn thi·∫øt cho containerd
  
	  - name: Activate kernel modules
		shell: |
		  modprobe overlay || true
		  modprobe br_netfilter || true
		# K√≠ch ho·∫°t module overlay v√† br_netfilter
  
	  - name: Configure sysctl parameters for Kubernetes
		copy:
		  dest: /etc/sysctl.d/99-kubernetes-cri.conf
		  content: |
			net.bridge.bridge-nf-call-iptables  = 1
			net.bridge.bridge-nf-call-ip6tables = 1
			net.ipv4.ip_forward                 = 1
		# C·∫•u h√¨nh sysctl cho networking Kubernetes
  
	  - name: Apply sysctl configuration
		command: sysctl --system
		# √Åp d·ª•ng c·∫•u h√¨nh sysctl
  
	  - name: Install containerd runtime
		apt:
		  name: containerd
		  state: present
		  update_cache: yes
		# C√†i ƒë·∫∑t containerd container runtime
  
	  - name: Generate default containerd config
		shell: |
		  mkdir -p /etc/containerd
		  containerd config default > /etc/containerd/config.toml
		# T·∫°o file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd
  
	  - name: Enable SystemdCgroup
		replace:
		  path: /etc/containerd/config.toml
		  regexp: 'SystemdCgroup = false'
		  replace: 'SystemdCgroup = true'
		# B·∫≠t SystemdCgroup trong containerd
  
	  - name: Restart and enable containerd
		systemd:
		  name: containerd
		  state: restarted
		  enabled: yes
		# Kh·ªüi ƒë·ªông l·∫°i v√† b·∫≠t containerd ƒë·ªÉ √°p d·ª•ng c·∫•u h√¨nh
  
  - name: Step 3 - Install Kubernetes core components
	hosts: all
	become: yes
	gather_facts: no
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Add Kubernetes GPG key
		shell: |
		  mkdir -p /usr/share/keyrings
		  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
		  gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
		changed_when: false
		ignore_errors: true
		# Th√™m GPG key ch√≠nh th·ª©c c·ªßa Kubernetes
  
	  - name: Add Kubernetes repository
		copy:
		  dest: /etc/apt/sources.list.d/kubernetes.list
		  content: |
			deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
		# Th√™m repository APT c·ªßa Kubernetes
  
	  - name: Install kubelet, kubeadm, kubectl
		apt:
		  name:
			- kubelet
			- kubeadm
			- kubectl
		  state: present
		  update_cache: yes
		# C√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn core c·ªßa Kubernetes
  
	  - name: Hold package version
		command: apt-mark hold kubelet kubeadm kubectl
		# Gi·ªØ phi√™n b·∫£n c√°c package ƒë·ªÉ tr√°nh c·∫≠p nh·∫≠t t·ª± ƒë·ªông
  
  - name: Step 4 - Initialize Master node
	hosts: master
	become: yes
	gather_facts: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Get master IP address
		set_fact:
		  master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
		# L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa master node
  
	  - name: Reset old control plane (if any)
		shell: kubeadm reset -f || true
		# Reset control plane c≈© n·∫øu c√≥
  
	  - name: Initialize Kubernetes control plane
		command: >
		  kubeadm init
		  --control-plane-endpoint "{{ master_ip }}:6443"
		  --apiserver-advertise-address {{ master_ip }}
		  --pod-network-cidr 10.244.0.0/16
		  --upload-certs
		args:
		  creates: /etc/kubernetes/admin.conf
		register: kubeadm_init
		failed_when: "'error' in kubeadm_init.stderr"
		changed_when: "'Your Kubernetes control-plane has initialized successfully' in kubeadm_init.stdout"
		# Kh·ªüi t·∫°o control plane Kubernetes v·ªõi Calico CNI
  
	  - name: Copy kubeconfig for root
		shell: |
		  mkdir -p /root/.kube
		  cp -i /etc/kubernetes/admin.conf /root/.kube/config
		  chown root:root /root/.kube/config
		args:
		  executable: /bin/bash
		# Sao ch√©p kubeconfig cho user root
  
	  - name: Copy kubeconfig for normal user
		when: ansible_user != "root"
		block:
		  - name: Create ~/.kube directory
			file:
			  path: "/home/{{ ansible_user }}/.kube"
			  state: directory
			  owner: "{{ ansible_user }}"
			  group: "{{ ansible_user }}"
			  mode: '0755'
			# T·∫°o th∆∞ m·ª•c kubeconfig cho user th∆∞·ªùng
  
		  - name: Copy kubeconfig file
			copy:
			  src: /etc/kubernetes/admin.conf
			  dest: "/home/{{ ansible_user }}/.kube/config"
			  remote_src: yes
			  owner: "{{ ansible_user }}"
			  group: "{{ ansible_user }}"
			  mode: '0600'
			# Sao ch√©p kubeconfig cho user th∆∞·ªùng
		# Sao ch√©p kubeconfig cho user th∆∞·ªùng (n·∫øu kh√¥ng ph·∫£i root)
  
  - name: Step 5 - Install Calico CNI
	hosts: master
	become: yes
	gather_facts: false
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
	vars:
	  calico_version: "v3.27.3"
	  calico_url: "https://raw.githubusercontent.com/projectcalico/calico/{{ calico_version }}/manifests/calico.yaml"
	tasks:
	  - name: Check if Calico CNI exists
		command: kubectl get daemonset calico-node -n kube-system
		register: calico_check
		ignore_errors: true
		# Ki·ªÉm tra Calico ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t tr∆∞·ªõc ƒë√≥
  
	  - name: Apply Calico manifest (install or update)
		command: kubectl apply -f {{ calico_url }}
		register: calico_apply
		retries: 3
		delay: 10
		until: calico_apply.rc == 0
		# √Åp d·ª•ng manifest Calico CNI
  
	  - name: Wait for Calico node pods to be Running
		shell: |
		  kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c 'Running' || true
		register: calico_running
		retries: 10
		delay: 15
		until: calico_running.stdout | int > 0
		# Ch·ªù c√°c pod Calico kh·ªüi ƒë·ªông v√† ·ªü tr·∫°ng th√°i Running
  
	  - name: Confirm Calico pods are active
		debug:
		  msg: "Calico is running ({{ calico_running.stdout }} pods Running)."
		# X√°c nh·∫≠n Calico ƒëang ho·∫°t ƒë·ªông
  
  - name: Step 6 - Join worker nodes
	hosts: workers
	become: yes
	gather_facts: false
	environment:
	  DEBIAN_FRONTEND: noninteractive
	vars:
	  join_script: /tmp/kube_join.sh
	tasks:
	  - name: Test SSH connectivity to worker node
		ping:
		register: ping_result
		ignore_errors: yes
		# Ki·ªÉm tra k·∫øt n·ªëi SSH ƒë·∫øn worker node
  
	  - name: Mark worker online status
		set_fact:
		  worker_online: "{{ ping_result is succeeded }}"
		# ƒê√°nh d·∫•u worker n√†o online/offline
  
	  - name: Display worker online status
		debug:
		  msg: "Worker {{ inventory_hostname }} is {{ 'ONLINE' if worker_online else 'OFFLINE' }}"
		# Hi·ªÉn th·ªã tr·∫°ng th√°i online/offline
  
	  - name: Retrieve join command from master
		delegate_to: "{{ groups['master'][0] }}"
		run_once: true
		shell: kubeadm token create --print-join-command
		register: join_cmd
		when: worker_online
		# L·∫•y l·ªánh join t·ª´ master node (ch·ªâ ch·∫°y 1 l·∫ßn)
  
	  - name: Save join command to script file
		copy:
		  content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
		  dest: "{{ join_script }}"
		  mode: '0755'
		when: worker_online
		ignore_errors: yes
		# L∆∞u l·ªánh join v√†o file script
  
	  - name: Reset old worker node
		shell: kubeadm reset -f || true
		ignore_errors: yes
		when: worker_online
		# Reset worker node c≈© (n·∫øu c√≥)
  
	  - name: Execute join command
		shell: "{{ join_script }}"
		register: join_output
		ignore_errors: yes
		when: worker_online
		# Th·ª±c thi l·ªánh join ƒë·ªÉ worker tham gia cluster
  
	  - name: Display join result summary
		debug:
		  msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"
		# B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng
  
  - name: Step 7 - Verify Kubernetes cluster status
	hosts: master
	become: yes
	gather_facts: false
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Check kubectl binary
		command: which kubectl
		register: kubectl_check
		failed_when: kubectl_check.rc != 0
		changed_when: false
		# Ki·ªÉm tra kubectl ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t
  
	  - name: List all nodes
		command: kubectl get nodes -o wide
		register: nodes_info
		changed_when: false
		# Li·ªát k√™ t·∫•t c·∫£ c√°c node trong cluster
  
	  - name: List system pods
		command: kubectl get pods -n kube-system -o wide
		register: pods_info
		changed_when: false
		# Li·ªát k√™ c√°c pod trong namespace kube-system
  
	  - name: Display cluster info
		debug:
		  msg:
			- "Node list:"
			- "{{ nodes_info.stdout_lines }}"
			- "Pods in kube-system namespace:"
			- "{{ pods_info.stdout_lines }}"
		# Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt v·ªÅ cluster
  
	  - name: Check node readiness
		shell: kubectl get nodes --no-headers | awk '{print $1, $2}' | column -t
		register: node_status
		changed_when: false
		# Ki·ªÉm tra tr·∫°ng th√°i Ready c·ªßa c√°c node
  
	  - name: Node status summary
		debug:
		  msg: |
			{% if 'NotReady' in node_status.stdout %}
			Some nodes are not ready:
			{{ node_status.stdout }}
			Please check kubelet or CNI (Calico) on those nodes.
			{% else %}
			All nodes are in Ready state!
			{{ node_status.stdout }}
			{% endif %}
		# T·ªïng k·∫øt tr·∫°ng th√°i node
  
	  - name: Detect problematic pods
		shell: kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' || true
		register: bad_pods
		changed_when: false
		# Ph√°t hi·ªán c√°c pod c√≥ v·∫•n ƒë·ªÅ
  
	  - name: Report problematic pods
		debug:
		  msg: |
			{% if bad_pods.stdout %}
			Some pods in kube-system are not stable:
			{{ bad_pods.stdout }}
			{% else %}
			All kube-system pods are Running or Completed!
			{% endif %}
		# B√°o c√°o c√°c pod c√≥ v·∫•n ƒë·ªÅ
  
	  - name: Collect logs from problematic pods
		when: bad_pods.stdout != ""
		shell: |
		  echo "==== Problematic Pod Logs ===="
		  for pod in $(kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' | awk '{print $1}'); do
			echo "---------------------------------------------"
			echo "Pod: $pod"
			kubectl logs -n kube-system $pod --tail=30 || echo "Cannot get logs for $pod"
			echo
		  done
		register: bad_pods_logs
		ignore_errors: yes
		# Thu th·∫≠p log c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ
  
	  - name: Display detailed logs
		when: bad_pods.stdout != ""
		debug:
		  msg: "{{ bad_pods_logs.stdout_lines | default(['No error logs or pods restarted.']) }}"
		# Hi·ªÉn th·ªã log chi ti·∫øt c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ`
	  ,
  
	  'deploy-full-cluster-flannel': `---
  - name: Step 0 - Reset Kubernetes cluster
	hosts: all
	become: yes
	gather_facts: yes
	environment:
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Reset old cluster if exists
		shell: kubeadm reset -f || true
		ignore_errors: true
		# Reset c·ª•m Kubernetes c≈© n·∫øu t·ªìn t·∫°i
  
	  - name: Remove old Kubernetes configuration
		file:
		  path: "{{ item }}"
		  state: absent
		loop:
		  - /etc/kubernetes
		  - /etc/cni/net.d
		  - /root/.kube
		  - "/home/{{ ansible_user }}/.kube"
		ignore_errors: true
		# X√≥a c√°c th∆∞ m·ª•c c·∫•u h√¨nh Kubernetes c≈©
  
	  - name: Restart containerd service
		shell: systemctl restart containerd || true
		ignore_errors: true
		# Kh·ªüi ƒë·ªông l·∫°i containerd ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch s·∫Ω
  
  - name: Step 1 - Update /etc/hosts and hostname
	hosts: all
	become: yes
	gather_facts: yes
	tasks:
	  - name: Update /etc/hosts file for all nodes
		lineinfile:
		  path: /etc/hosts
		  line: "{{ hostvars[item].ansible_host }} {{ item }}"
		  state: present
		  create: yes
		  insertafter: EOF
		loop: "{{ groups['all'] }}"
		when: hostvars[item].ansible_host is defined
		# Th√™m danh s√°ch c√°c node v√†o /etc/hosts ƒë·ªÉ cluster nh·∫≠n di·ªán nhau
  
	  - name: Set hostname according to inventory
		hostname:
		  name: "{{ inventory_hostname }}"
		when: ansible_hostname != inventory_hostname
		# ƒê·∫∑t hostname theo t√™n trong inventory
  
	  - name: Verify hostname
		shell: hostnamectl
		register: host_info
		# Ki·ªÉm tra hostname ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t ƒë√∫ng
  
	  - name: Display hostname info
		debug:
		  msg: "{{ host_info.stdout_lines }}"
		# Hi·ªÉn th·ªã th√¥ng tin hostname
  
  - name: Step 2 - Configure kernel and containerd
	hosts: all
	become: yes
	gather_facts: no
	tasks:
	  - name: Disable swap
		shell: swapoff -a && sed -i '/swap/d' /etc/fstab || true
		# T·∫Øt swap v√¨ Kubernetes kh√¥ng h·ªó tr·ª£
  
	  - name: Load kernel modules for containerd
		copy:
		  dest: /etc/modules-load.d/containerd.conf
		  content: |
			overlay
			br_netfilter
		# T·∫£i c√°c kernel module c·∫ßn thi·∫øt cho containerd
  
	  - name: Activate kernel modules
		shell: |
		  modprobe overlay || true
		  modprobe br_netfilter || true
		# K√≠ch ho·∫°t module overlay v√† br_netfilter
  
	  - name: Configure sysctl parameters for Kubernetes
		copy:
		  dest: /etc/sysctl.d/99-kubernetes-cri.conf
		  content: |
			net.bridge.bridge-nf-call-iptables  = 1
			net.bridge.bridge-nf-call-ip6tables = 1
			net.ipv4.ip_forward                 = 1
		# C·∫•u h√¨nh sysctl cho networking Kubernetes
  
	  - name: Apply sysctl configuration
		command: sysctl --system
		# √Åp d·ª•ng c·∫•u h√¨nh sysctl
  
	  - name: Install containerd runtime
		apt:
		  name: containerd
		  state: present
		  update_cache: yes
		# C√†i ƒë·∫∑t containerd container runtime
  
	  - name: Generate default containerd config
		shell: |
		  mkdir -p /etc/containerd
		  containerd config default > /etc/containerd/config.toml
		# T·∫°o file c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho containerd
  
	  - name: Enable SystemdCgroup
		replace:
		  path: /etc/containerd/config.toml
		  regexp: 'SystemdCgroup = false'
		  replace: 'SystemdCgroup = true'
		# B·∫≠t SystemdCgroup trong containerd
  
	  - name: Restart and enable containerd
		systemd:
		  name: containerd
		  state: restarted
		  enabled: yes
		# Kh·ªüi ƒë·ªông l·∫°i v√† b·∫≠t containerd ƒë·ªÉ √°p d·ª•ng c·∫•u h√¨nh
  
  - name: Step 3 - Install Kubernetes core components
	hosts: all
	become: yes
	gather_facts: no
	tasks:
	  - name: Add Kubernetes GPG key
		shell: |
		  mkdir -p /usr/share/keyrings
		  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
			gpg --dearmor --yes -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
		changed_when: false
		ignore_errors: true
		# Th√™m GPG key ch√≠nh th·ª©c c·ªßa Kubernetes
  
	  - name: Add Kubernetes repository
		copy:
		  dest: /etc/apt/sources.list.d/kubernetes.list
		  content: |
			deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /
		# Th√™m repository APT c·ªßa Kubernetes
  
	  - name: Install kubelet, kubeadm, kubectl
		apt:
		  name:
			- kubelet
			- kubeadm
			- kubectl
		  state: present
		  update_cache: yes
		# C√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn core c·ªßa Kubernetes
  
	  - name: Hold package version
		command: apt-mark hold kubelet kubeadm kubectl
		# Gi·ªØ phi√™n b·∫£n c√°c package ƒë·ªÉ tr√°nh c·∫≠p nh·∫≠t t·ª± ƒë·ªông
  
  - name: Step 4 - Initialize Master node
	hosts: master
	become: yes
	gather_facts: yes
	tasks:
	  - name: Get master IP address
		set_fact:
		  master_ip: "{{ hostvars[inventory_hostname].ansible_host | default(ansible_default_ipv4.address) }}"
		# L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa master node
  
	  - name: Reset old control plane (if any)
		shell: kubeadm reset -f || true
		# Reset control plane c≈© n·∫øu c√≥
  
	  - name: Initialize Kubernetes control plane
		command: >
		  kubeadm init
		  --control-plane-endpoint "{{ master_ip }}:6443"
		  --apiserver-advertise-address {{ master_ip }}
		  --pod-network-cidr 10.244.0.0/16
		args:
		  creates: /etc/kubernetes/admin.conf
		register: kubeadm_init
		failed_when: "'error' in kubeadm_init.stderr"
		changed_when: "'Your Kubernetes control-plane has initialized successfully' in kubeadm_init.stdout"
		# Kh·ªüi t·∫°o control plane Kubernetes v·ªõi Flannel CNI
  
	  - name: Copy kubeconfig for root
		shell: |
		  mkdir -p /root/.kube
		  cp -i /etc/kubernetes/admin.conf /root/.kube/config
		  chown root:root /root/.kube/config
		args:
		  executable: /bin/bash
		# Sao ch√©p kubeconfig cho user root
  
	  - name: Copy kubeconfig for normal user
		when: ansible_user != "root"
		block:
		  - name: Create ~/.kube directory
			file:
			  path: "/home/{{ ansible_user }}/.kube"
			  state: directory
			  owner: "{{ ansible_user }}"
			  group: "{{ ansible_user }}"
			  mode: '0755'
			# T·∫°o th∆∞ m·ª•c kubeconfig cho user th∆∞·ªùng
  
		  - name: Copy kubeconfig file
			copy:
			  src: /etc/kubernetes/admin.conf
			  dest: "/home/{{ ansible_user }}/.kube/config"
			  remote_src: yes
			  owner: "{{ ansible_user }}"
			  group: "{{ ansible_user }}"
			  mode: '0600'
			# Sao ch√©p kubeconfig cho user th∆∞·ªùng
		# Sao ch√©p kubeconfig cho user th∆∞·ªùng (n·∫øu kh√¥ng ph·∫£i root)
  
  - name: Step 5 - Install Flannel CNI
	hosts: master
	become: yes
	gather_facts: false
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	  DEBIAN_FRONTEND: noninteractive
	tasks:
	  - name: Apply Flannel CNI manifest
		command: kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
		register: flannel_apply
		changed_when: "'created' in flannel_apply.stdout or 'configured' in flannel_apply.stdout"
		# √Åp d·ª•ng manifest Flannel CNI
  
	  - name: Wait for Flannel pods to be Running
		shell: |
		  kubectl get pods -n kube-flannel --no-headers 2>/dev/null | grep -c 'Running' || true
		register: flannel_running
		retries: 10
		delay: 15
		until: flannel_running.stdout | int > 0
		# Ch·ªù c√°c pod Flannel kh·ªüi ƒë·ªông v√† ·ªü tr·∫°ng th√°i Running
  
	  - name: Confirm Flannel pods are active
		debug:
		  msg: "Flannel is running ({{ flannel_running.stdout }} pods Running)."
		# X√°c nh·∫≠n Flannel ƒëang ho·∫°t ƒë·ªông
  
  - name: Step 6 - Join worker nodes
	hosts: workers
	become: yes
	gather_facts: false
	vars:
	  join_script: /tmp/kube_join.sh
	tasks:
	  - name: Test SSH connectivity to worker node
		ping:
		register: ping_result
		ignore_errors: yes
		# Ki·ªÉm tra k·∫øt n·ªëi SSH ƒë·∫øn worker node
  
	  - name: Mark worker online status
		set_fact:
		  worker_online: "{{ ping_result is succeeded }}"
		# ƒê√°nh d·∫•u worker n√†o online/offline
  
	  - name: Display worker online status
		debug:
		  msg: "Worker {{ inventory_hostname }} is {{ 'ONLINE' if worker_online else 'OFFLINE' }}"
		# Hi·ªÉn th·ªã tr·∫°ng th√°i online/offline
  
	  - name: Retrieve join command from master
		delegate_to: "{{ groups['master'][0] }}"
		run_once: true
		shell: kubeadm token create --print-join-command
		register: join_cmd
		when: worker_online
		# L·∫•y l·ªánh join t·ª´ master node (ch·ªâ ch·∫°y 1 l·∫ßn)
  
	  - name: Save join command to script file
		copy:
		  content: "{{ join_cmd.stdout }} --ignore-preflight-errors=all"
		  dest: "{{ join_script }}"
		  mode: '0755'
		when: worker_online
		ignore_errors: yes
		# L∆∞u l·ªánh join v√†o file script
  
	  - name: Reset old worker node
		shell: kubeadm reset -f || true
		ignore_errors: yes
		when: worker_online
		# Reset worker node c≈© (n·∫øu c√≥)
  
	  - name: Execute join command
		shell: "{{ join_script }}"
		register: join_output
		ignore_errors: yes
		when: worker_online
		# Th·ª±c thi l·ªánh join ƒë·ªÉ worker tham gia cluster
  
	  - name: Display join result summary
		debug:
		  msg: "{{ 'Node ' + inventory_hostname + ' ƒë√£ tham gia c·ª•m th√†nh c√¥ng!' if worker_online else 'Worker ' + inventory_hostname + ' OFFLINE - B·ªè qua join' }}"
		# B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng
  
  - name: Step 7 - Verify Kubernetes cluster status
	hosts: master
	become: yes
	gather_facts: false
	environment:
	  KUBECONFIG: /etc/kubernetes/admin.conf
	tasks:
	  - name: Check kubectl binary
		command: which kubectl
		register: kubectl_check
		failed_when: kubectl_check.rc != 0
		changed_when: false
		# Ki·ªÉm tra kubectl ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t
  
	  - name: List all nodes
		command: kubectl get nodes -o wide
		register: nodes_info
		changed_when: false
		# Li·ªát k√™ t·∫•t c·∫£ c√°c node trong cluster
  
	  - name: List system pods
		command: kubectl get pods -n kube-system -o wide
		register: pods_info
		changed_when: false
		# Li·ªát k√™ c√°c pod trong namespace kube-system
  
	  - name: Display cluster info
		debug:
		  msg:
			- "Node list:"
			- "{{ nodes_info.stdout_lines }}"
			- "Pods in kube-system namespace:"
			- "{{ pods_info.stdout_lines }}"
		# Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt v·ªÅ cluster
  
	  - name: Check node readiness
		shell: kubectl get nodes --no-headers | awk '{print $1, $2}' | column -t
		register: node_status
		changed_when: false
		# Ki·ªÉm tra tr·∫°ng th√°i Ready c·ªßa c√°c node
  
	  - name: Node status summary
		debug:
		  msg: |
			{% if 'NotReady' in node_status.stdout %}
			Some nodes are not ready:
			{{ node_status.stdout }}
			Please check kubelet or CNI (Flannel) on those nodes.
			{% else %}
			All nodes are in Ready state!
			{{ node_status.stdout }}
			{% endif %}
		# T·ªïng k·∫øt tr·∫°ng th√°i node
  
	  - name: Detect problematic pods
		shell: kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' || true
		register: bad_pods
		changed_when: false
		# Ph√°t hi·ªán c√°c pod c√≥ v·∫•n ƒë·ªÅ
  
	  - name: Report problematic pods
		debug:
		  msg: |
			{% if bad_pods.stdout %}
			Some pods in kube-system are not stable:
			{{ bad_pods.stdout }}
			{% else %}
			All kube-system pods are Running or Completed!
			{% endif %}
		# B√°o c√°o c√°c pod c√≥ v·∫•n ƒë·ªÅ
  
	  - name: Collect logs from problematic pods
		when: bad_pods.stdout != ""
		shell: |
		  echo "==== Problematic Pod Logs ===="
		  for pod in $(kubectl get pods -n kube-system --no-headers | grep -vE 'Running|Completed' | awk '{print $1}'); do
			echo "---------------------------------------------"
			echo "Pod: $pod"
			kubectl logs -n kube-system $pod --tail=30 || echo "Cannot get logs for $pod"
			echo
		  done
		register: bad_pods_logs
		ignore_errors: yes
		# Thu th·∫≠p log c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ
  
	  - name: Display detailed logs
		when: bad_pods.stdout != ""
		debug:
		  msg: "{{ bad_pods_logs.stdout_lines | default(['No error logs or pods restarted.']) }}"
		# Hi·ªÉn th·ªã log chi ti·∫øt c·ªßa c√°c pod c√≥ v·∫•n ƒë·ªÅ
  `
	};
  
	// Map template value (without numbers) to template key (with numbers)
	const templateMapping = {
	  'update-hosts-hostname': '01-update-hosts-hostname',
	  'kernel-sysctl': '02-kernel-sysctl',
	  'install-containerd': '03-install-containerd',
	  'install-kubernetes': '04-install-kubernetes',
	  'init-master': '05-init-master',
	  'install-cni': '06-install-cni',
	  'install-flannel': '06-install-flannel',
	  'join-workers': '07-join-workers',
	  'verify-cluster': '08-verify-cluster',
	  'install-ingress': '09-install-ingress',
	  'install-metallb': '10-install-metallb',
	  'install-helm': '11-install-helm',
	  'setup-storage': '12-setup-storage',
	  'prepare-and-join-worker': '13-prepare-and-join-worker',
	  'deploy-full-cluster': 'deploy-full-cluster',
	  'deploy-full-cluster-flannel': 'deploy-full-cluster-flannel',
	  'reset-cluster': '00-reset-cluster'
	};
  
	// Get the actual template key
	const actualTemplate = templateMapping[template] || template;
  
	const playbookContent = templates[actualTemplate];
	if (!playbookContent) {
	  throw new Error('Template kh√¥ng t·ªìn t·∫°i');
	}
  
	const filename = actualTemplate + '.yml';
  
	// Check if playbook already exists
	const exists = await checkPlaybookExists(filename);
	console.log(`Checking if ${filename} exists:`, exists);
  
	if (exists) {
	  const templateNames = {
		'00-reset-cluster': 'Reset to√†n b·ªô cluster',
		'01-update-hosts-hostname': 'C·∫≠p nh·∫≠t hosts & hostname',
		'02-kernel-sysctl': 'C·∫•u h√¨nh kernel & sysctl',
		'03-install-containerd': 'C√†i ƒë·∫∑t Containerd',
		'04-install-kubernetes': 'C√†i ƒë·∫∑t Kubernetes',
		'05-init-master': 'Kh·ªüi t·∫°o Master',
		'06-install-cni': 'C√†i CNI (Calico)',
		'06-install-flannel': 'C√†i CNI (Flannel)',
		'07-join-workers': 'Join Workers',
		'08-verify-cluster': 'X√°c minh tr·∫°ng th√°i c·ª•m',
		'09-install-ingress': 'C√†i Ingress Controller',
		'10-install-metallb': 'C√†i MetalLB LoadBalancer',
		'11-install-helm': 'C√†i Helm',
		'12-setup-storage': 'Setup Storage',
		'13-prepare-and-join-worker': 'Chu·∫©n b·ªã & Join Worker (02‚Üí03‚Üí04‚Üí07)',
		'deploy-full-cluster': 'Tri·ªÉn khai to√†n b·ªô cluster (Calico)',
		'deploy-full-cluster-flannel': 'Tri·ªÉn khai to√†n b·ªô cluster (Flannel)'
	  };
  
	  const templateName = templateNames[actualTemplate] || actualTemplate;
	  const confirmMessage = `Playbook "${filename}" (${templateName}) ƒë√£ t·ªìn t·∫°i. B·∫°n c√≥ mu·ªën ghi ƒë√® l√™n file c≈©?`;
  
	  console.log('Showing confirm dialog for:', confirmMessage);
  
	  if (!confirm(confirmMessage)) {
		console.log('User cancelled overwrite');
		throw new Error('ƒê√£ h·ªßy t·∫°o playbook t·ª´ template');
	  }
  
	  console.log('User confirmed overwrite');
	}
  
	// Thay th·∫ø cluster_id trong n·ªôi dung playbook n·∫øu c√≥
	let finalPlaybookContent = playbookContent;
	if (getClusterId() && playbookContent.includes('{{ cluster_id | default(1) }}')) {
	  finalPlaybookContent = playbookContent.replace('{{ cluster_id | default(1) }}', getClusterId());
	}
  
	try {
	  const result = await fetch(`/api/ansible-playbook/save/${getClusterId()}`, {
		method: 'POST',
		headers: {
		  'Content-Type': 'application/x-www-form-urlencoded',
		},
		body: `filename=${encodeURIComponent(filename)}&content=${encodeURIComponent(finalPlaybookContent)}`
	  });
  
	  if (!result.ok) {
		const errorData = await result.json();
		throw new Error(errorData.error || 'L·ªói t·∫°o playbook');
	  }
  
	  return await result.json();
	} catch (error) {
	  console.error('L·ªói t·∫°o playbook K8s:', error);
	  throw error;
	}
  }
  
  // Hi·ªÉn th·ªã n·ªôi dung playbook
  function showPlaybookContentView() {
	const contentArea = document.getElementById('playbook-content-area');
	const executionArea = document.getElementById('playbook-execution-status');
  
	if (contentArea) contentArea.style.display = 'block';
	if (executionArea) executionArea.style.display = 'none';
  }
  
  // Hi·ªÉn th·ªã th·ª±c thi playbook
  function showPlaybookExecutionView() {
	const contentArea = document.getElementById('playbook-content-area');
	const executionArea = document.getElementById('playbook-execution-status');
  
	if (contentArea) contentArea.style.display = 'none';
	if (executionArea) executionArea.style.display = 'block';
  }
  
  // T√¨m ki·∫øm playbook
  function searchPlaybooks(query) {
	const items = document.querySelectorAll('#playbook-list .playbook-item');
	const searchTerm = query.toLowerCase().trim();
  
	items.forEach(item => {
	  const playbookName = item.textContent.toLowerCase();
	  const shouldShow = playbookName.includes(searchTerm);
  
	  const listItem = item.closest('.list-group-item');
	  if (listItem) {
		listItem.style.display = shouldShow ? 'flex' : 'none';
	  }
	});
  }
  
  // C·∫≠p nh·∫≠t danh s√°ch playbook
  window.refreshPlaybooks = async function () {
	const refreshBtn = document.getElementById('refresh-playbooks-btn');
	if (refreshBtn) {
	  refreshBtn.disabled = true;
	  refreshBtn.innerHTML = '<span class="spinner-border spinner-border-sm me-2"></span>ƒêang t·∫£i...';
	}
  
	try {
	  await loadPlaybooks();
	} catch (error) {
	  console.error('Error refreshing playbooks:', error);
	  showAlert('error', 'L·ªói l√†m m·ªõi danh s√°ch playbook');
	} finally {
	  if (refreshBtn) {
		refreshBtn.disabled = false;
		refreshBtn.innerHTML = 'L√†m m·ªõi';
	  }
	}
  };
  
  // ƒê·∫∑t ID cluster hi·ªán t·∫°i
  function setCurrentClusterId(clusterId) {
	currentClusterId = clusterId;
  }
  
  // Xu·∫•t c√°c h√†m cho truy c·∫≠p to√†n c·ª•c
  window.loadPlaybook = loadPlaybook;
  window.savePlaybook = savePlaybook;
  window.deletePlaybook = deletePlaybook;
  window.executePlaybook = executePlaybook;
  window.uploadPlaybook = uploadPlaybook;
  window.refreshPlaybooks = refreshPlaybooks;
  window.searchPlaybooks = searchPlaybooks;
  window.showPlaybookContentView = showPlaybookContentView;
  window.showPlaybookExecutionView = showPlaybookExecutionView;
  window.setCurrentClusterId = setCurrentClusterId;
  window.generateK8sPlaybookFromTemplate = generateK8sPlaybookFromTemplate;
  
  // Reset giao di·ªán Playbook Manager khi r·ªùi kh·ªèi trang chi ti·∫øt cluster
  window.resetPlaybookUI = function () {
	try {
	  const list = document.getElementById('playbook-list');
	  if (list) list.innerHTML = '';
	  const fn = document.getElementById('playbook-filename');
	  if (fn) fn.value = '';
	  const ed = document.getElementById('playbook-editor');
	  if (ed) ed.value = '';
	  const contentArea = document.getElementById('playbook-content-area');
	  const execArea = document.getElementById('playbook-execution-status');
	  if (contentArea) contentArea.style.display = 'none';
	  if (execArea) execArea.style.display = 'none';
	  const output = document.getElementById('ansible-output');
	  if (output) output.textContent = '';
	  const alertBox = document.getElementById('playbook-alert');
	  if (alertBox) alertBox.innerHTML = '';
	  const search = document.getElementById('playbook-search');
	  if (search) search.value = '';
	} catch (_) { }
  }
  